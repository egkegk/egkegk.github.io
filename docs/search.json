[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Below you may find a PDF of my CV, which details more of my past experience.\nDownload PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Below is a portfolio of data analysis, visualization, and mathematical research projects I have worked on.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 12, 2025\n\n\nMCMC Sampling from a Multimodal Density\n\n\nEmrys King\n\n\n\n\n\n\nDec 12, 2025\n\n\nLinear and Nonlinear Models with a Quantile Loss Function\n\n\nEmrys King\n\n\n\n\n\n\nMay 5, 2025\n\n\nThe Legacy of Eugenics in Statistics Pedagogy (Senior Thesis in Mathematics)\n\n\nEmrys King\n\n\n\n\n\n\nMar 29, 2025\n\n\nData Science for Social Impact: Global Tuberculosis Mortality\n\n\nCharlotte Imbert and Emrys King\n\n\n\n\n\n\nDec 6, 2024\n\n\nPrediction of Life Expectancy and its Relationship to GDP\n\n\nEmrys King and Marissa Douglas\n\n\n\n\n\n\nNov 26, 2024\n\n\nPrediction of Global Temperatures with Climate Proxy Variables\n\n\nEmrys King\n\n\n\n\n\n\nDec 4, 2023\n\n\nEfficacy of STEM Community Learning Practices\n\n\nEmrys King, Atharv Kulkarni, and Tiernan Colby\n\n\n\n\n\n\nJul 28, 2023\n\n\nLeaky Positive Semidefinite Forcing on Graphs\n\n\nOlivia Elias, Ian Farish, Emrys King, Josh Kyei, Ryan Moruzzi, Jr.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/csmlexam-reg.html",
    "href": "projects/csmlexam-reg.html",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "",
    "text": "Let \\(\\ell\\) be the asymmetric quantile loss function defined by \\[\n\\ell_{\\tau}(y,q) = \\begin{cases}\n  \\tau(y-q) & y &gt; q \\\\\n  (\\tau-1)(y-q) & y \\leq q\n\\end{cases}\n\\] where \\(\\tau\\in (0,1)\\) is the quantile level. Let \\(R_\\tau (q;x) = \\mathbb{E}[\\ell_\\tau(Y,q)\\mid X=x]\\) denote the conditional risk and note that \\(q^*_\\tau :=\\arg\\min_qR_\\tau(q;x)\\) is the conditional \\(\\tau\\)-quantile of \\(Y\\mid X=x\\).\nThroughout this analysis, I will be using the Boston dataset from the MASS package, modelling response medv (median house price in $1000s) by predictors lstat (percent lower status of the population) and rm (average number of rooms per dwelling). The data (\\(n=506\\)) has been randomly split into a training set (\\(n_{\\textrm{train}}=354\\)) and a testing set (\\(n_{\\textrm{test}}=152\\)) prior to any analysis."
  },
  {
    "objectID": "projects/csmlexam-reg.html#sec-int",
    "href": "projects/csmlexam-reg.html#sec-int",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "",
    "text": "Let \\(\\ell\\) be the asymmetric quantile loss function defined by \\[\n\\ell_{\\tau}(y,q) = \\begin{cases}\n  \\tau(y-q) & y &gt; q \\\\\n  (\\tau-1)(y-q) & y \\leq q\n\\end{cases}\n\\] where \\(\\tau\\in (0,1)\\) is the quantile level. Let \\(R_\\tau (q;x) = \\mathbb{E}[\\ell_\\tau(Y,q)\\mid X=x]\\) denote the conditional risk and note that \\(q^*_\\tau :=\\arg\\min_qR_\\tau(q;x)\\) is the conditional \\(\\tau\\)-quantile of \\(Y\\mid X=x\\).\nThroughout this analysis, I will be using the Boston dataset from the MASS package, modelling response medv (median house price in $1000s) by predictors lstat (percent lower status of the population) and rm (average number of rooms per dwelling). The data (\\(n=506\\)) has been randomly split into a training set (\\(n_{\\textrm{train}}=354\\)) and a testing set (\\(n_{\\textrm{test}}=152\\)) prior to any analysis."
  },
  {
    "objectID": "projects/csmlexam-reg.html#sec-lin",
    "href": "projects/csmlexam-reg.html#sec-lin",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "Linear Models for Varying Quantiles",
    "text": "Linear Models for Varying Quantiles\nI implemented the loss function and its empirical mean as loss() and empirical_loss(), which can be found in Section 8. Let us now consider the baseline linear model given by \\[\nf^{(0)}_\\tau(x) = \\beta_{0,\\tau}^{(0)} + \\beta_{1,\\tau}^{(0)}\\texttt{lstat} + \\beta_{2,\\tau}^{(0)}\\texttt{rm}\n\\]\nI used the optim function to minimise \\(\\ell_\\tau(y,f^{(0)}_\\tau(x))\\) over the training dataset for each \\(\\tau \\in \\{0.1,0.5,0.9\\}\\). Specifically, I used the Nelder-Mead method, considered to be robust (as it does not require gradients) while at times slow. In this scenario, where the training set is small, I am not particularly worried about relative slowness, since all processes will be quick; I thus prioritised the robustness of Nelder-Mead. Since Nelder-Mead can be sensitive to its starting values, I began the optimisation algorithm at \\((\\beta_{0,\\tau},\\beta_{1,\\tau},\\beta_{2,\\tau})=(\\bar y, 0, 0)\\), where \\(y=\\texttt{medv}\\) and thus \\(\\bar y\\) is the sample mean of medv. These starting parameters represent the typical null hypothesis of linear modelling, in which the predictors (here, lstat and rm) have no influence on the response (here, medv), and so the best linear model is simply a horizontal line at the sample mean of the response. After running the optim for each value of \\(\\tau\\in\\{0.1,0.5,0.9\\}\\), we find the coefficients presented in Table 1.\n\n\n\n\nTable 1: The estimated coefficients \\(\\hat\\beta_{i,\\tau}^{(0)}\\) that minimise the empirical loss over the training set.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\hat \\beta_{0,\\tau}^{(0)}\\)\n\\(\\hat \\beta_{1,\\tau}^{(0)}\\)\n\\(\\hat \\beta_{2,\\tau}^{(0)}\\)\n\n\n\n\n0.1\n16.147\n-0.805\n1.787\n\n\n0.5\n-8.700\n-0.592\n6.038\n\n\n0.9\n-20.000\n-0.299\n8.419\n\n\n\n\n\n\n\n\nTo evaluate the linear models’ performance, we would like a characterisation of the empirical loss against the test data. This can be done by calculating the empirical risk, \\(\\hat R_{\\tau}(f_\\tau^{(0)};x_\\mathrm{test})\\), which is simply the mean loss of the models under the test data. We can compare this to the empirical risk against the training data to get a sense of over/underfitting. The results of this calculation are shown in Table 2. The values are not sufficiently different to cause concern.\n\n\n\n\nTable 2: The mean empirical loss (i.e., the empirical risk) of the linear model over the test dataset for \\(\\tau \\in {0.1,0.5,0.9}\\).\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(0)};x_{\\mathrm{train}})\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(0)};x_{\\mathrm{test}})\\)\n\n\n\n\n0.1\n0.685\n0.745\n\n\n0.5\n1.953\n1.859\n\n\n0.9\n1.231\n1.032\n\n\n\n\n\n\n\n\nFinally, we would like to gather an impression the accuracy of the quantile calculation in this model. Let \\[\n\\hat C_\\tau^{(0)}=\\frac{1}{n_\\mathrm{test}}\\sum_{i\\in\\mathrm{test}}\\mathbf{1}_{\\left\\{Y_i\\leq f_\\tau^{(0)}(X_i)\\right\\}}\n\\] denote the empirical coverage, which measures the proportion of test responses which lie below the \\(\\tau\\)-quantile. For well-calibrated models, \\(\\hat C_\\tau^{(0)}\\approx \\tau\\). Thus, we can evaluate the calibration of the baseline linear models by calculating their empirical coverages, which can be found in Table 3. Since these values are each near the true (respective) value of \\(\\tau\\), it is clear that the model is well-calibrated.\n\n\n\n\nTable 3: The empirical coverage of the linear model for \\(\\tau\\in{0.1,0.5,0.9}\\).\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\hat C_{\\tau}^{(0)}\\)\n\n\n\n\n0.1\n0.112\n\n\n0.5\n0.513\n\n\n0.9\n0.928"
  },
  {
    "objectID": "projects/csmlexam-reg.html#sec-nonlin",
    "href": "projects/csmlexam-reg.html#sec-nonlin",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "Nonlinear Models for Varying Quantiles",
    "text": "Nonlinear Models for Varying Quantiles\nNow, let us consider the following nonlinear model: \\[\nf_\\tau^{(1)}(x) = \\beta_{0,\\tau}^{(1)} + \\beta_{1,\\tau}^{(1)}\\texttt{lstat} + \\beta_{2,\\tau}^{(1)}\\texttt{rm} + \\beta_{3,\\tau}^{(1)}\\texttt{lstat}^2 + \\beta_{4,\\tau}^{(1)}\\texttt{rm}^2 + \\beta_{5,\\tau}^{(1)}\\texttt{lstat}\\times\\texttt{rm}\n\\] Similarly to the linear models, I use optim with Nelder-Mead and starting values \\((\\beta_{0,\\tau}^{(1)},\\beta_{1,\\tau}^{(1)},\\beta_{2,\\tau}^{(1)}, \\beta_{3,\\tau}^{(1)},\\beta_{4,\\tau}^{(1)}, \\beta_{5,\\tau}^{(1)})\\) \\(=(\\bar y, 0, 0,0,0,0)\\) to minimise the average empirical loss (or, empirical risk) over the training dataset. The optimised coefficients are presented in Table 4.\n\n\n\n\nTable 4: The coefficient estimates of the nonlinear model that minimise the average empirical loss over the training data, for \\(\\tau\\in{0.1,0.5,0.9}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\hat \\beta_{0,\\tau}^{(1)}\\)\n\\(\\hat \\beta_{1,\\tau}^{(1)}\\)\n\\(\\hat \\beta_{2,\\tau}^{(1)}\\)\n\\(\\hat \\beta_{3,\\tau}^{(1)}\\)\n\\(\\hat \\beta_{4,\\tau}^{(1)}\\)\n\\(\\hat \\beta_{5,\\tau}^{(1)}\\)\n\n\n\n\n0.1\n23.952\n-0.539\n-0.786\n0.014\n0.273\n-0.109\n\n\n0.5\n20.382\n-0.800\n0.464\n0.024\n0.336\n-0.125\n\n\n0.9\n26.595\n-0.322\n0.809\n0.054\n0.409\n-0.358\n\n\n\n\n\n\n\n\nNow, to evaluate this model, we look again at the average empirical loss and the empirical coverage on the test data. These calculations are included in Table 5. The empirical risk does not change drastically between training and testing for any \\(\\tau\\), meaning that overfitting has been avoided. Furthermore, the empirical coverage is near the true value of \\(\\tau\\) for all \\(\\tau\\), indicating the model is well-calibrated.\n\n\n\n\nTable 5: The empirical risk (for training and testing data) and coverage of the nonlinear model for \\(\\tau\\in{0.1,0.5,0.9}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(1)};X_{\\textrm{train}})\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(1)};X_{\\textrm{test}})\\)\n\\(\\hat C_{\\tau}^{(1)}\\)\n\n\n\n\n0.1\n0.654\n0.688\n0.105\n\n\n0.5\n1.741\n1.687\n0.553\n\n\n0.9\n0.980\n0.917\n0.901"
  },
  {
    "objectID": "projects/csmlexam-reg.html#sec-ridge",
    "href": "projects/csmlexam-reg.html#sec-ridge",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "Ridge Regression for Varying Quantiles",
    "text": "Ridge Regression for Varying Quantiles\nNow, I implement a ridge regression model. This model penalises large coefficients in order to reduce model complexity. In particular, the coefficients solve the following equation: \\[\n\\hat \\beta_{\\tau,\\lambda}^{\\textrm{ridge}}= \\arg\\min_{\\beta}\\widehat R_{\\tau, \\lambda}^\\mathrm{ridge}(\\beta)\n\\] where \\(\\beta\\) is a vector of coefficients and \\(\\widehat R_{\\tau, \\lambda}^\\mathrm{ridge}(\\beta)\\) is the ridge-penalised empirical risk given by \\[\n\\widehat R_{\\tau, \\lambda}^\\mathrm{ridge}(\\beta) = \\frac{1}{n_\\mathrm{train}}\\sum_{i\\in\\mathrm{train}}\\ell_\\tau(Y_i, f_\\tau^{(1)}(X_i;\\beta)) + \\lambda\\|\\beta\\|^2_2.\n\\] where \\(\\lambda &gt; 0\\) is meant to regularise the coefficients and \\(\\|\\cdot\\|_2\\) denotes the \\(\\ell_2\\) norm. In other words, we are now optimising \\(\\beta\\) over the mean loss (the same as in the previous sections) summed with a penalisation term, \\(\\lambda\\|\\beta\\|^2\\).\nSetting an appropriate value of \\(\\lambda\\) takes some fine-tuning. Thus, I implement a \\(k\\)-fold cross-validation algorithm to select an optimal \\(\\lambda\\in[10^{-4},10^{-1}]\\) for each \\(\\tau\\). Using \\(k=5\\) folds on each candidate value of \\(\\lambda\\), I iteratively fit the ridge-penalised model over the training data, minimising the ridge-penalised empirical risk using optim as described in previous sections. I then selected the value of \\(\\lambda\\) that minimised the mean risk over all \\(k\\)-folds, and refit the model using this value of \\(\\lambda\\). The coefficients for the ridge-penalised model (at each value of \\(\\tau\\)) may be found in Table 6, along with the \\(\\lambda\\) selected through cross-validation. For each \\(\\tau\\), the \\(\\lambda\\) selected is the left endpoint of the candidate region, \\(10^{-4}\\). This means the penalisation for larger coefficients in model is very small, and thus the ridge-penalisation does not cause a major difference in the fit compared to the nonlinear model from Section 3.\n\n\n\n\nTable 6: The \\(\\lambda\\) selected for each value of \\(\\tau\\) using \\(k\\)-fold cross-validation, and the coefficient estimates for the corresponding ridge-penalised nonlinear model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\lambda_{\\textrm{min}}\\)\n\\(\\hat \\beta_{0;\\tau,\\lambda_{\\textrm{min}}}^{\\textrm{ridge}}\\)\n\\(\\hat \\beta_{1;\\tau,\\lambda_{\\textrm{min}}}^{\\textrm{ridge}}\\)\n\\(\\hat \\beta_{2;\\tau,\\lambda_{\\textrm{min}}}^{\\textrm{ridge}}\\)\n\\(\\hat \\beta_{3;\\tau,\\lambda_{\\textrm{min}}}^{\\textrm{ridge}}\\)\n\\(\\hat \\beta_{4;\\tau,\\lambda_{\\textrm{min}}}^{\\textrm{ridge}}\\)\n\\(\\hat \\beta_{5;\\tau,\\lambda_{\\textrm{min}}}^{\\textrm{ridge}}\\)\n\n\n\n\n0.1\n0.0001\n23.6932\n-0.4684\n-0.4703\n0.0139\n0.2359\n-0.1261\n\n\n0.5\n0.0001\n12.2976\n-0.2026\n0.9951\n0.0263\n0.4695\n-0.2359\n\n\n0.9\n0.0001\n26.9021\n0.0781\n-0.4700\n0.0411\n0.5533\n-0.3607\n\n\n\n\n\n\n\n\nThe average empirical test loss, average empirical training loss, and the empirical coverage of the ridge-penalised model are included in Table 7. Once again, there appears to be no overfitting, and the empirical coverages approximate the true values of \\(\\tau\\).\n\n\n\n\nTable 7: The empirical risk (for training and testing data) and coverage of the ridge-penalised model for \\(\\tau\\in{0.1,0.5,0.9}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\widehat R_{\\tau,\\lambda}^{\\mathrm{ridge}}(\\hat\\beta_{\\tau,\\lambda_\\mathrm{min}};x_{\\mathrm{test}})\\)\n\\(\\widehat R_{\\tau,\\lambda}^{\\mathrm{ridge}}(\\hat\\beta_{\\tau,\\lambda_\\mathrm{min}};x_{\\mathrm{train}})\\)\n\\(\\widehat C_{\\tau}^{\\mathrm{ridge}}\\)\n\n\n\n\n0.1\n0.749\n0.713\n0.105\n\n\n0.5\n1.673\n1.741\n0.513\n\n\n0.9\n0.935\n1.034\n0.908"
  },
  {
    "objectID": "projects/csmlexam-reg.html#comparison",
    "href": "projects/csmlexam-reg.html#comparison",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "Comparison",
    "text": "Comparison\nIn this section, I compare the three models constructed above with regards to coeﬃcient magnitudes, test loss, overfitting and empirical coverage.\nThe coefficients for the linear, nonlinear, and ridge-penalised models are available in Table 1, Table 4, and Table 6, respectively. For each of these models, the actual coefficient values can be sensitive to the seed used to randomly split the full dataset into training and testing datasets. This is most pertinent with regards to the constant term, \\(\\hat\\beta_{0,\\tau}\\), which can have very different magnitudes depending on what observations are used to train the model; however, while \\(\\hat\\beta_{0,\\tau}^{(0)}\\) can be negative depending on training data and quantile level, the nonlinear model and ridge-penalised model tend to produce positive constant terms. These latter estimates are potentially more realistic, as a negative median house value is illogical. Beyond the constant term, some general observations can be made on coefficient magnitude regardless of training data. The baseline linear model typically has much larger non-constant coefficients compared to the nonlinear and ridge-penalised models. As discussed in Section 4, the small value of \\(\\lambda\\) in the ridge-penalised model indicates that a ridge regularisation did not largely improve the nonlinear model fit, and thus there does not tend to be a major difference between these two models’ coefficients.\nThe average empirical test loss of each model is compared in Table 8. Some of these values are repeated from Table 2, Table 5, and Table 7 for ease of comparison. Though the precise values are sensitive to the randomised testing/training split, the risk tends to decrease from the baseline to the nonlinear model, especially for larger values of \\(\\tau\\). The ridge penalisation does not greatly affect the empirical risk positively, and in some cases the empirical risk actually increases with the addition of the risk penalisation.\n\n\n\n\nTable 8: Comparison of average empirical loss of each model on the testing dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(0)};x_{\\mathrm{test}})\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(1)};x_{\\mathrm{test}})\\)\n\\(\\widehat R_{\\tau,\\lambda}^{\\mathrm{ridge}}(\\hat\\beta_{\\tau,\\lambda_\\mathrm{min}};x_{\\mathrm{test}})\\)\n\n\n\n\n0.1\n0.745\n0.688\n0.749\n\n\n0.5\n1.859\n1.687\n1.673\n\n\n0.9\n1.032\n0.917\n0.935\n\n\n\n\n\n\n\n\nTo evaluate overfitting, we can compare the average empirical loss over the training set with that over the testing set. Thus, I include the average empirical training loss in Table 9. Once again, some of these values are repeated from other tables. For each model, there is not a great change in values between Table 9 and Table 8. This indicates that no overfitting is occurring, as the models are robust enough to evaluate unseen data with the same level of accuracy as the data they were trained on.\n\n\n\n\nTable 9: Comparison of average empirical loss of each model on the training dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(0)};x_{\\mathrm{train}})\\)\n\\(\\hat R_{\\tau}(f_{\\tau}^{(1)};x_{\\mathrm{train}})\\)\n\\(\\widehat R_{\\tau,\\lambda}^{\\mathrm{ridge}}(\\hat\\beta_{\\tau,\\lambda_\\mathrm{min}};x_{\\mathrm{train}})\\)\n\n\n\n\n0.1\n0.685\n0.654\n0.713\n\n\n0.5\n1.953\n1.741\n1.741\n\n\n0.9\n1.231\n0.980\n1.034\n\n\n\n\n\n\n\n\nFinally, we evaluate the empirical coverage of each model for each value of \\(\\tau\\). The coverage values are included in Table 10. The ridge-penalised model tends to have empirical coverage nearest the true value of \\(\\tau\\), but all models are well-calibrated and thus have empirical coverage approximately equal to \\(\\tau\\).\n\n\n\n\nTable 10: Comparison of empirical coverage of each model.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau\\)\n\\(\\widehat C_{\\tau}^{(0)}\\)\n\\(\\widehat C_{\\tau}^{(1)}\\)\n\\(\\widehat C_{\\tau}^{\\mathrm{ridge}}\\)\n\n\n\n\n0.1\n0.112\n0.105\n0.105\n\n\n0.5\n0.513\n0.553\n0.513\n\n\n0.9\n0.928\n0.901\n0.908\n\n\n\n\n\n\n\n\nOverall, if I had to pick only one model to use, I would choose the nonlinear model, \\(f_\\tau^{(1)}\\). This model had better performance than the linear model, and the slightly better performance of the ridge-penalised model in some cases is not necessarily worth the added computational complexity necessary of the latter."
  },
  {
    "objectID": "projects/csmlexam-reg.html#theoretical-guarantees-of-quantiles",
    "href": "projects/csmlexam-reg.html#theoretical-guarantees-of-quantiles",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "Theoretical Guarantees of Quantiles",
    "text": "Theoretical Guarantees of Quantiles\nThe theoretical (true) quantiles, \\(q_\\tau(x)\\), never cross, meaning \\(q_\\tau(x)&lt;q_{\\tau'}(x)\\) for all \\(x\\) so long as \\(\\tau,\\tau'\\in(0,1)\\) and \\(\\tau &lt; \\tau'\\). However, this does not necessarily occur in the quantile regression implemented in this question thus far. The average empirical loss is only minimised for a single value of \\(\\tau\\). Thus, in its current state, the model has no way of minimising with regard to other quantiles, let alone minimising to ensure that quantile estimates never cross.\nIn order to train a model on each \\(\\tau \\in \\{0.1, 0.5, 0.9\\}\\) that does ensure quantile estimates never (or at least rarely) cross, we need a risk function that accounts for the possibility of crossing and penalises it. Assume we have multi-output model \\(f= (f_{\\tau_1}, f_{\\tau_2}, \\dots, f_{\\tau_N})\\) that we would like to fit. Then, as a risk function, I propose the empirical mean loss summed with a hinge penalisation (see Rosasco et al. (2004) p. 1067) term on crossings: \\[\n\\widehat R_{\\tau, \\lambda}^\\mathrm{cross}(\\beta) = \\frac{1}{n_\\mathrm{train}}\\sum_{i\\in\\mathrm{train}}\\left(\\ell(Y_i, f (X_i;\\beta)) + \\lambda\\sum_{j = 1}^N\\max\\{(f_{\\tau_{j}}(X_i;\\beta)-f_{\\tau_{j+1}}(X_i;\\beta)), 0\\}\\right)\n\\] where \\(\\lambda &gt;0\\) is a regularisation constant and the \\(\\ell(y,q)=(\\ell_{\\tau_1}(y, q),\\ell_{\\tau_2}(y, q),\\dots,\\ell_{\\tau_N}(y, q))\\) is a multi-output loss function composed of each individual asymmetric loss (following the definition presented at the beginning of Question 2). Essentially, this risk function would at once fit the asymmetric quantile loss function while penalising any fits that allow crossing. The \\(\\max\\{f_{\\tau_j}-f_{\\tau_{j+1}}, 0\\}\\) term will evaluate to 0 when crossing does not occur, giving no penalty, but evaluates to a positive number when crossing does occur. Thus, we can penalise crossing while using the same loss function. This is a version of a hinge loss function, used in classification models, but here incorporated as a penalising term (Rosasco et al. 2004).\nTo implement this for \\(\\tau\\in \\{0.1,0.5,0.9\\}\\), I would run a similar \\(k\\)-fold cross-validation to the one constructed in Section 4 to find the optimal value of \\(\\lambda\\) across the set of \\(\\tau\\) values. I would then fit the model for the given \\(\\lambda\\), minimising the risk \\(\\widehat R_{\\tau, \\lambda}^\\mathrm{cross}(\\beta)\\). This should result in a model that minimises crossing as much as possible."
  },
  {
    "objectID": "projects/csmlexam-reg.html#references",
    "href": "projects/csmlexam-reg.html#references",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "References",
    "text": "References\n\n\nRosasco, Lorenzo, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. 2004. “Are Loss Functions All the Same?” Neural Computation 16 (5): 1063–76. https://doi.org/10.1162/089976604773135104."
  },
  {
    "objectID": "projects/csmlexam-reg.html#sec-code",
    "href": "projects/csmlexam-reg.html#sec-code",
    "title": "Linear and Nonlinear Models with a Quantile Loss Function",
    "section": "Code Appendix",
    "text": "Code Appendix\n\n# Document setup\nknitr::opts_chunk$set(\n  echo = FALSE,\n  collapse = TRUE,\n  comment = \"#&gt;\",\n  dev = \"ragg_png\"\n)\nlibrary(tidyverse)\nlibrary(kableExtra)\n# Question 2\nset.seed(4747)\ndata(\"Boston\", package=\"MASS\")\nmodel_data &lt;- subset(Boston, select = c(\"medv\", \"lstat\", \"rm\"))\ntrain_size &lt;- floor(0.7 * nrow(Boston))\nidx &lt;- sample(1:nrow(Boston), train_size)\nboston_train &lt;- model_data[idx,]\nboston_test &lt;- model_data[-idx,]\n# part (a)\nloss &lt;- function(tau,y,q){\n  ans &lt;- ifelse(y &gt; q, tau*(y-q), (tau-1)*(y-q))\n  return(ans)\n}\n\nempirical_loss &lt;- function(coefs, data, quantile){\n  beta0 &lt;- coefs[1]\n  beta1 &lt;- coefs[2]\n  beta2 &lt;- coefs[3]\n  \n  q &lt;- beta0 + beta1 * data$lstat + beta2 * data$rm\n  mean(loss(quantile, data$medv, q))\n}\n# bullet 1\ntaus &lt;- c(0.1, 0.5, 0.9)\ncoefs_init &lt;- c(mean(boston_train$medv), 0, 0)\nopt_res &lt;- lapply(taus,\n                  function(tau){optim(par = coefs_init,\n                                      fn = empirical_loss,\n                                      data = boston_train,\n                                      quantile = tau,\n                                      method = \"Nelder-Mead\")\n                    })\nopt_res_table &lt;- data.frame(tau = taus,\n                            beta0 = sapply(1:3,\n                                           function(i) opt_res[[i]]$par[[1]]),\n                            beta1 = sapply(1:3,\n                                           function(i) opt_res[[i]]$par[[2]]),\n                            beta2 = sapply(1:3,\n                                           function(i) opt_res[[i]]$par[[3]]))\nknitr::kable(opt_res_table,\n             col.names = c(\"$\\\\tau$\",\n                           \"$\\\\hat \\\\beta_{0,\\\\tau}^{(0)}$\",\n                           \"$\\\\hat \\\\beta_{1,\\\\tau}^{(0)}$\",\n                           \"$\\\\hat \\\\beta_{2,\\\\tau}^{(0)}$\"),\n             digits = 3)\n# bullet 2\nopt_res_test &lt;- data.frame(\n  tau = taus,\n  emp_loss_train = sapply(1:3, function(i){\n    model_coefs &lt;- opt_res[[i]]$par\n    empirical_loss(model_coefs, boston_train, taus[[i]])\n}),\n  emp_loss_test = sapply(1:3, function(i){\n    model_coefs &lt;- opt_res[[i]]$par\n    empirical_loss(model_coefs, boston_test, taus[[i]])\n}))\n\nknitr::kable(opt_res_test,\n             col.names = c(\"$\\\\tau$\",\n             \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(0)};x_{\\\\mathrm{train}})$\",\n             \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(0)};x_{\\\\mathrm{test}})$\"),\n             digits = 3)\nempirical_coverage &lt;- function(coefs, data){\n  beta0 &lt;- coefs[1]\n  beta1 &lt;- coefs[2]\n  beta2 &lt;- coefs[3]\n  \n  q &lt;- beta0 + beta1 * data$lstat + beta2 * data$rm\n  y &lt;- data$medv\n  mean(sapply(1:length(y), function(i) y[[i]] &lt;= q[[i]]))\n}\n\ncoverages &lt;- data.frame(\n  tau = taus,\n  emp_cov = sapply(1:3, function(i){\n    model_coefs &lt;- opt_res[[i]]$par\n    empirical_coverage(model_coefs, boston_test)\n}))\n\nknitr::kable(coverages,\n             col.names = c(\"$\\\\tau$\",\n                           \"$\\\\hat C_{\\\\tau}^{(0)}$\"),\n             digits = 3)\nnonlinear_model_data &lt;- model_data %&gt;%\n  mutate(lstatsq = I(lstat^2),\n         rmsq = I(rm^2),\n         lstatrm = I(lstat*rm))\n\nnonlin_train &lt;- nonlinear_model_data[idx,]\nnonlin_test &lt;- nonlinear_model_data[-idx,]\n\nnonlin_emp_loss &lt;- function(coefs, data, quantile){\n  beta0 &lt;- coefs[1]\n  beta1 &lt;- coefs[2]\n  beta2 &lt;- coefs[3]\n  beta3 &lt;- coefs[4]\n  beta4 &lt;- coefs[5]\n  beta5 &lt;- coefs[6]\n  \n  q &lt;- beta0 + beta1 * data$lstat + beta2 * data$rm + beta3 * data$lstatsq +\n    beta4 * data$rmsq + beta5 * data$lstatrm\n  mean(loss(quantile, data$medv, q))\n}\n  \n\ncoefs_init &lt;- c(mean(nonlin_train$medv),0,0,0,0,0)\nnonlin_opt &lt;- lapply(taus,\n                  function(tau){optim(par = coefs_init,\n                                      fn = nonlin_emp_loss,\n                                      data = nonlin_train,\n                                      quantile = tau,\n                                      method = \"Nelder-Mead\")\n                    })\n\nnonlin_table &lt;- data.frame(tau = taus,\n                           beta0 = sapply(1:3,\n                                          function(i) nonlin_opt[[i]]$par[[1]]),\n                           beta1 = sapply(1:3,\n                                          function(i) nonlin_opt[[i]]$par[[2]]),\n                           beta2 = sapply(1:3,\n                                          function(i) nonlin_opt[[i]]$par[[3]]),\n                           beta3 = sapply(1:3,\n                                          function(i) nonlin_opt[[i]]$par[[4]]),\n                           beta4 = sapply(1:3,\n                                          function(i) nonlin_opt[[i]]$par[[5]]),\n                           beta5 = sapply(1:3,\n                                          function(i) nonlin_opt[[i]]$par[[6]]))\n\nknitr::kable(nonlin_table,\n             col.names = c(\"$\\\\tau$\",\n                           \"$\\\\hat \\\\beta_{0,\\\\tau}^{(1)}$\",\n                           \"$\\\\hat \\\\beta_{1,\\\\tau}^{(1)}$\",\n                           \"$\\\\hat \\\\beta_{2,\\\\tau}^{(1)}$\",\n                           \"$\\\\hat \\\\beta_{3,\\\\tau}^{(1)}$\",\n                           \"$\\\\hat \\\\beta_{4,\\\\tau}^{(1)}$\",\n                           \"$\\\\hat \\\\beta_{5,\\\\tau}^{(1)}$\"),\n             digits = 3)\nnonlin_cover &lt;- function(coefs, data){\n  beta0 &lt;- coefs[1]\n  beta1 &lt;- coefs[2]\n  beta2 &lt;- coefs[3]\n  beta3 &lt;- coefs[4]\n  beta4 &lt;- coefs[5]\n  beta5 &lt;- coefs[6]\n  \n  q &lt;- beta0 + beta1 * data$lstat + beta2 * data$rm + beta3 * data$lstatsq +\n    beta4 * data$rmsq + beta5 * data$lstatrm\n  y &lt;- data$medv\n  mean(sapply(1:length(y), function(i) y[[i]] &lt;= q[[i]]))\n}\n\nnonlin_test_table &lt;- data.frame(\n  tau = taus,\n  train_loss = sapply(1:3, function(i){\n    model_coefs &lt;- nonlin_opt[[i]]$par\n    nonlin_emp_loss(model_coefs, nonlin_train, taus[[i]])}),\n  test_loss = sapply(1:3, function(i){\n    model_coefs &lt;- nonlin_opt[[i]]$par\n    nonlin_emp_loss(model_coefs, nonlin_test, taus[[i]])}),\n  emp_cov = sapply(1:3, function(i){\n    model_coefs &lt;- nonlin_opt[[i]]$par\n    nonlin_cover(model_coefs, nonlin_test)\n}))\n\nknitr::kable(nonlin_test_table,\n             col.names = c(\n               \"$\\\\tau$\",\n               \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(1)};X_{\\\\textrm{train}})$\",\n               \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(1)};X_{\\\\textrm{test}})$\",\n               \"$\\\\hat C_{\\\\tau}^{(1)}$\"),\n             digits = 3)\n# defining ridge regression risk estimator\nridge_risk &lt;- function(coefs, data, tau, lambda){\n  beta0 &lt;- coefs[1]\n  beta1 &lt;- coefs[2]\n  beta2 &lt;- coefs[3]\n  beta3 &lt;- coefs[4]\n  beta4 &lt;- coefs[5]\n  beta5 &lt;- coefs[6]\n  \n  q &lt;- beta0 + beta1 * data$lstat + beta2 * data$rm + beta3 * data$lstatsq +\n    beta4 * data$rmsq + beta5 * data$lstatrm\n  mean_loss &lt;- mean(loss(tau, data$medv, q))\n  penalisation &lt;- lambda * sum(coefs^2)\n  mean_loss + penalisation\n}\n\n# k-fold cross-validation for lambdas\nkfold_cv &lt;- function(kfold, candidates, tau){\n  fold &lt;- sample(rep(1:kfold, length.out = train_size))\n  cv_risk &lt;- numeric(length(candidates))\n  \n  for (lambda in candidates){\n    errs &lt;- numeric(kfold)\n    for (j in 1:kfold) {\n      tr &lt;- fold != j\n      va &lt;- fold == j\n\n      # build training and validation data frames with consistent variable names\n      df_tr &lt;- nonlin_train[tr,]\n      df_va &lt;- nonlin_train[va,]\n\n      # fit model using consistent variable names\n      fit &lt;- optim(par = coefs_init,\n                 fn = ridge_risk,\n                 data = df_tr,\n                 tau = tau,\n                 lambda = lambda,\n                 method = \"Nelder-Mead\")\n\n      # predict using consistent names\n      errs[j] &lt;- ridge_risk(fit$par, df_va, tau, lambda) ## NEW RISK HERE\n    }\n  cv_risk[lambda] &lt;- mean(errs)\n  }\n  best_lambda &lt;- candidates[which.min(cv_risk)]\n  best_fit &lt;- optim(par = coefs_init,\n                 fn = ridge_risk,\n                 data = nonlin_train,\n                 tau = tau,\n                 lambda = best_lambda,\n                 method = \"Nelder-Mead\")\n  return(c(best_lambda, best_fit))\n}\n\nlambdas &lt;- seq(1e-4, 1e-1, length.out = 100)\nridge_results &lt;- lapply(taus,\n                        function(tau) kfold_cv(5, lambdas, tau))\n\noptions(scipen = 999)\nridge_table &lt;- data.frame(tau = taus,\n             lambda = sapply(1:3,\n                             function(i) ridge_results[[i]][[1]]),\n             beta0 = sapply(1:3,\n                            function(i) ridge_results[[i]]$par[[1]]),\n             beta1 = sapply(1:3,\n                            function(i) ridge_results[[i]]$par[[2]]),\n             beta2 = sapply(1:3,\n                            function(i) ridge_results[[i]]$par[[3]]),\n             beta3 = sapply(1:3,\n                            function(i) ridge_results[[i]]$par[[4]]),\n             beta4 = sapply(1:3,\n                            function(i) ridge_results[[i]]$par[[5]]),\n             beta5 = sapply(1:3,\n                            function(i) ridge_results[[i]]$par[[6]]))\n\nknitr::kable(ridge_table,\n             col.names = c(\"$\\\\tau$\",\n                           \"$\\\\lambda_{\\\\textrm{min}}$\",\n       \"$\\\\hat \\\\beta_{0;\\\\tau,\\\\lambda_{\\\\textrm{min}}}^{\\\\textrm{ridge}}$\",\n       \"$\\\\hat \\\\beta_{1;\\\\tau,\\\\lambda_{\\\\textrm{min}}}^{\\\\textrm{ridge}}$\",\n       \"$\\\\hat \\\\beta_{2;\\\\tau,\\\\lambda_{\\\\textrm{min}}}^{\\\\textrm{ridge}}$\",\n       \"$\\\\hat \\\\beta_{3;\\\\tau,\\\\lambda_{\\\\textrm{min}}}^{\\\\textrm{ridge}}$\",\n       \"$\\\\hat \\\\beta_{4;\\\\tau,\\\\lambda_{\\\\textrm{min}}}^{\\\\textrm{ridge}}$\",\n       \"$\\\\hat \\\\beta_{5;\\\\tau,\\\\lambda_{\\\\textrm{min}}}^{\\\\textrm{ridge}}$\"),\n       digits = 4)\nridgeeval &lt;- data.frame(tau = taus,\n                        testloss = sapply(1:3, function(i){\n                        model_coefs &lt;- ridge_results[[i]]$par\n                        ridge_risk(model_coefs, nonlin_test, taus[[i]],\n                                   lambda = 1e-4)}),\n                        trainloss = sapply(1:3, function(i){\n                        model_coefs &lt;- ridge_results[[i]]$par\n                        ridge_risk(model_coefs, nonlin_train, taus[[i]],\n                                   lambda = 1e-4)}),\n                        emp_cov = sapply(1:3, function(i){\n                        model_coefs &lt;- ridge_results[[i]]$par\n                        nonlin_cover(model_coefs, nonlin_test)}))\n\nknitr::kable(ridgeeval,\n             col.names = c(\"$\\\\tau$\",\n\"$\\\\widehat R_{\\\\tau,\\\\lambda}^{\\\\mathrm{ridge}}(\\\\hat\\\\beta_{\\\\tau,\\\\lambda_\\\\mathrm{min}};x_{\\\\mathrm{test}})$\",\n\"$\\\\widehat R_{\\\\tau,\\\\lambda}^{\\\\mathrm{ridge}}(\\\\hat\\\\beta_{\\\\tau,\\\\lambda_\\\\mathrm{min}};x_{\\\\mathrm{train}})$\",\n\"$\\\\widehat C_{\\\\tau}^{\\\\mathrm{ridge}}$\"),\ndigits = 3)\ntestlosscomp &lt;- data.frame(tau = taus,\n                           baseline = opt_res_test$emp_loss_test,\n                           nonlinear = nonlin_test_table$test_loss,\n                           ridge = ridgeeval$testloss)\n\nknitr::kable(testlosscomp,\n             col.names = c(\"$\\\\tau$\",\n               \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(0)};x_{\\\\mathrm{test}})$\",\n               \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(1)};x_{\\\\mathrm{test}})$\",\n\"$\\\\widehat R_{\\\\tau,\\\\lambda}^{\\\\mathrm{ridge}}(\\\\hat\\\\beta_{\\\\tau,\\\\lambda_\\\\mathrm{min}};x_{\\\\mathrm{test}})$\"\n             ),\ndigits = 3)\ntestlosscomp &lt;- data.frame(tau = taus,\n                           baseline = opt_res_test$emp_loss_train,\n                           nonlinear = nonlin_test_table$train_loss,\n                           ridge = ridgeeval$trainloss)\n\nknitr::kable(testlosscomp,\n             col.names = c(\"$\\\\tau$\",\n               \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(0)};x_{\\\\mathrm{train}})$\",\n               \"$\\\\hat R_{\\\\tau}(f_{\\\\tau}^{(1)};x_{\\\\mathrm{train}})$\",\n\"$\\\\widehat R_{\\\\tau,\\\\lambda}^{\\\\mathrm{ridge}}(\\\\hat\\\\beta_{\\\\tau,\\\\lambda_\\\\mathrm{min}};x_{\\\\mathrm{train}})$\"\n             ),\ndigits = 3)\ntestlosscomp &lt;- data.frame(tau = taus,\n                           baseline = coverages$emp_cov,\n                           nonlinear = nonlin_test_table$emp_cov,\n                           ridge = ridgeeval$emp_cov)\n\nknitr::kable(testlosscomp,\n             col.names = c(\"$\\\\tau$\",\n               \"$\\\\widehat C_{\\\\tau}^{(0)}$\",\n               \"$\\\\widehat C_{\\\\tau}^{(1)}$\",\n               \"$\\\\widehat C_{\\\\tau}^{\\\\mathrm{ridge}}$\"\n             ),\n             digits = 3)"
  },
  {
    "objectID": "projects/suami.html",
    "href": "projects/suami.html",
    "title": "Leaky Positive Semidefinite Forcing on Graphs",
    "section": "",
    "text": "I was a member of the 2023 Summer Undergraduate Applied Mathematics Institute at Carnegie Mellon University, under the mentorship of Ryan Moruzzi, Jr. and Michael Young. My group studied a novel variant of zero forcing. Our poster was accepted for presentation at the 2023 National Diversity in STEM (NDiSTEM)/SACNAS Conference, where it received an award for Excellence in Undergraduate Mathematics Poster Presentation, and at the 2024 Joint Mathematics Meetings (JMM) in San Francisco. I was a presenting author at both conferences.\nThe results of our research have since been published in Involve: A Journal of Mathematics. The journal copy of our paper is available here, and an arXiv preprint can be found here.\nDownload PDF file."
  },
  {
    "objectID": "projects/SLMMidtermExam.html",
    "href": "projects/SLMMidtermExam.html",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "",
    "text": "In this report, I will be analyzing the dataset globwarm, which reports the Average Northern Hemisphere Temperature from 1856-2000 and eight climate proxies from 1000-2000AD. Using the information from the climate proxies above I will attempt to predict the global temperature in years predating 1856, before which temperatures are unreliable due to the lack of standardization of thermometers. To most accurately predict some of these years, I will build seven different linear regression models and evaluate each in a holistic, comprehensive manner.\nThus, the question I am hoping to answer is: which linear model is best? To answer this question, I will first conduct an exploratory analysis of the dataset and establish the periodicity of the data when viewed from a time series perspective. I will then fit a linear model using ordinary least squares to estimate the parameters \\(\\beta\\), evaluate the need for a transformation of the response variable using Box-Cox, and communicate the parameter estimates and standard errors of the model and their meaning. Finally, I will test the assumptions of ordinary least squares against each model, find influential points in the dataset that may be affecting the fit, and establish the effect of collinearity on the model. In the conclusion, I will communicate overall findings, consider possible best models and the drawbacks to each, and ultimately predict the northern hemisphere temperature in two of the unknown years, 1850 and 1000."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#data-description-and-exploratory-analysis",
    "href": "projects/SLMMidtermExam.html#data-description-and-exploratory-analysis",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "1. Data Description and Exploratory Analysis",
    "text": "1. Data Description and Exploratory Analysis\nTo begin, I include here a description of the variables in the dataset globwarm.\n\n\nVariableDescriptionnhtempNorthern hemisphere average temperature (C) provided by the UK Met Office (known as HadCRUT2)wusaTree ring proxy information from the Western USA.jasperTree ring proxy information from Canada.westgreenIce core proxy information from west GreenlandchesapeakeSea shell proxy information from Chesapeake BaytornetraskTree ring proxy information from SwedenuralsTree ring proxy information from the UralsmongoliaTree ring proxy information from MongoliatasmanTree ring proxy information from TasmaniayearYear 1000-2000AD\n\n\nSince the goal of this analysis is to predict temperature using the other eight climate variables, it is first necessary to obtain a subset of the data from 1856 onwards, since only these years include data on temperature.\nAll model building will be based on the dataset globwarm1856, since this dataset is (i.e., not NA) for all values of nhtemp, no matter the year.\nTo begin, let us visually observe the relationship between year and average global temperature. Included is the standard linear model of best fit in blue, and the kernel-smoothed loess curve of best fit in red.\n\n\n\n\n\n\n\n\n\nThe above graph shows that, though the data certainly has a curve to it, a linear model could be used to fit the relationship between nhtemp and year. In particular, though the residuals do not have constant variance (observe the right tail of the datapoints compared to the other datapoints), a transformation of the response, nhtemp, may result in the meeting of this assumption. Furthermore, in the most general terms, this graph shows that the average temperature in the Northern Hemisphere has been increasing as the years go on.\nTo further explore the dataset, observe the following graphs of tree growth dependent on year and average temperature. “Tree growth” is agglomerated from all tree ring proxy variables — the source of each datapoint (i.e., the variable it is associated with) is depicted via its color.\n\n\n\n\n\n\n\n\n\nThe above graph, “Tree Growth by Year,” most importantly displays the periodicity of tree growth over time. This pattern indicates that autoregression modelling methods will likely be most appropriate for any model that includes year as a predictor and any tree growth proxy as the response.\n\n\n\n\n\n\n\n\n\nThe above graph shows the truth of the hypothesis guiding this analysis, that information on the rate of tree growth indicates that tree growth increases in warmer temperatures. The slope of the line of best fit is positive, which means that as temperature increases, so does the rate of tree growth.\nTo conclude the exploratory data analysis, observe the following summary of year in relation to every other variable in the dataset.\n\n\n\n\n\n\n\n\n\nAs noted with the “Tree Growth by Year” plot above, the most important trend to be gleaned is the periodicity of each variable in relation to year. This periodicity is something that cannot accurately be modelled other than by autoregression. For the majority of the analysis, year will be removed from the predictors to be able to use standard linear regression techniques. However, in the following subsection, the autoregressive model predicting wusa, the tree ring proxy information from the western United States, using lagged variables."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#autoregression",
    "href": "projects/SLMMidtermExam.html#autoregression",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "2. Autoregression",
    "text": "2. Autoregression\nTo begin, let us focus on the plot of wusa vs. year, with the standard linear model \\(\\texttt{wusa}_i=\\hat\\beta_0+\\hat\\beta_1\\texttt{year}_i\\) overlaid.\n\n\n\n\n\n\n\n\n\nThe periodicity of the plot above violates the assumptions of linear regression: instead of independently and identically distributed residuals centered around a constant, the residuals will follow the periodic nature of the data itself. To account for the time series nature of the given data, one may instead build an autoregressive model by embedding “lagged” data as predictors of a dataframe. The nature of lagged data may be seen below — note how the value of wusa_og in a given observation becomes the value of lag1 in the subsequent observation, and how the value of lag1 in a given observation becomes the value of lag2 in the subsequent observation, and so on.\n\n\n  wusa_og  lag1  lag2  lag3\n1   -0.85 -0.84 -0.81 -0.78\n2   -0.84 -0.85 -0.84 -0.81\n3   -0.79 -0.84 -0.85 -0.84\n4   -0.72 -0.79 -0.84 -0.85\n5   -0.62 -0.72 -0.79 -0.84\n6   -0.52 -0.62 -0.72 -0.79\n\n\nUsing the lagdf dataset, we may predict the value of wusa given its previous values, a technique known as autoregression.\n\n\n\nCall:\nlm(formula = wusa_og ~ ., data = data.frame(lagdf))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0311282 -0.0074819 -0.0000792  0.0078532  0.0298416 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.003150   0.001333   2.363   0.0195 *  \nlag1         2.624708   0.061706  42.536   &lt;2e-16 ***\nlag2        -2.311777   0.121175 -19.078   &lt;2e-16 ***\nlag3         0.683676   0.060810  11.243   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01139 on 138 degrees of freedom\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 2.085e+05 on 3 and 138 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the above summary, one sees that the fit of the autoregressive model predicting wusa is nearly perfect: the \\(R^2\\) value is 0.9998, with the maximum possible \\(R^2\\) being 1. The fit of the model can be seen visually in the plot below. The original wusa data is included as a black solid line, while the autoregressive fit is included as a red dashed line.\n\n\n\n\n\n\n\n\n\nIt is thus clear that the periodic nature of the wusa data over time is captured by the autoregressive model. The problem with the assumptions that appeared in the standard linear model does not occur here; the periodicity of the data will not be replicated in the residuals.\nNow, to employ the autoregression as a predictive model, one must first observe the final entry of lagdf dataset, which corresponds to the data for the year 2000.\n\n\n    wusa_og lag1 lag2 lag3\n142    0.88 0.84 0.82 0.82\n\n\nDue to the nature of the construction of the lagged data, the values of wusa_og, lag1, and lag2 may be plugged in for lag1, lag2, and lag3, respectively to predict the value of wusa in 2001. These calculations are accomplished below, and the 95% confidence interval is included in the results.\n\n\n        fit       lwr       upr\n1 0.9316155 0.9088515 0.9543796\n\n\n[1] 0.6089223\n\n\nThus, the expected value of wusa in 2001 is 0.9316155, with the 95% confidence interval being \\((0.9088515, 0.9543796)\\). This confidence interval is a bit wide given the spread of the data: in globwarm1856, the variance of wusa is approximately 0.6, which means that a confidence interval of width just under 0.5 is almost equal to the variance of the data.\nSimilar to the method above, to predict the value of wusa in 2002, one may plug in the values of wusa_og, and lag1 from the last row of lagdf for lag2 and lag3, while the expected value for 2001 calculated above may be used as the estimate for lag1. The calculation and confidence interval are thus returned.\n\n\n        fit       lwr      upr\n1 0.9958969 0.9731721 1.018622\n\n\nThe expected value of wusa in 2002 is 0.9958969, with the 95% confidence interval being \\((0.9731721, 1.018622)\\). The width of this confidence interval is approximately 0.3, with is smaller than the width of the previous confidence interval and approximately half of the variance of wusa in globwarm1956.\nAutoregression techniques are useful for predicting explicitly time-series data, like tree ring proxy information predicted by year. However, using the climate proxy information in isolation from year, one may build a standard linear regression model without using autoregression. Models of this form will be built and evaluated in the following sections."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#full-model-and-stepwise-elimination",
    "href": "projects/SLMMidtermExam.html#full-model-and-stepwise-elimination",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "3. Full Model and Stepwise Elimination",
    "text": "3. Full Model and Stepwise Elimination\nTo begin, observe the following standard linear regression model, which assigns nhtemp as the response and all eight climate proxies as predictors. This model will be referred to as the “full model,” the linear regression model with no transformations and all possible predictors (except year).\n\n\n\nCall:\nlm(formula = nhtemp ~ wusa + jasper + westgreen + chesapeake + \n    tornetrask + urals + mongolia + tasman, data = globwarm1856)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43668 -0.11170  0.00698  0.10176  0.65352 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.242555   0.027011  -8.980 1.97e-15 ***\nwusa         0.077384   0.042927   1.803 0.073647 .  \njasper      -0.228795   0.078107  -2.929 0.003986 ** \nwestgreen    0.009584   0.041840   0.229 0.819168    \nchesapeake  -0.032112   0.034052  -0.943 0.347346    \ntornetrask   0.092668   0.045053   2.057 0.041611 *  \nurals        0.185369   0.091428   2.027 0.044567 *  \nmongolia     0.041973   0.045794   0.917 0.360996    \ntasman       0.115453   0.030111   3.834 0.000192 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1758 on 136 degrees of freedom\nMultiple R-squared:  0.4764,    Adjusted R-squared:  0.4456 \nF-statistic: 15.47 on 8 and 136 DF,  p-value: 5.028e-16\n\n\nFrom the summary above, one observes half of the variables as significant (jasper, tornetrask, urals, and tasman), with a poor \\(R^2\\) value of 0.4764. The fit of this model may be improved using stepwise regression, which sequentially compares models via their Aikake’s Important Criterion (AIC), defined elimination \\(AIC=-2L(\\hat\\theta)+2p\\), where \\(-2L(\\hat\\theta)=n\\log{(RSS/n)}+c\\) for sample size \\(n\\), number of parameters \\(p\\), residual sum of squares \\(RSS\\), and some constant \\(c\\). This process ceases upon finding the best selection of predictors without a transformation of the response.\n\n\nStart:  AIC=-495.48\nnhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask + \n    urals + mongolia + tasman\n\n             Df Sum of Sq    RSS     AIC\n- westgreen   1   0.00162 4.2033 -497.43\n- mongolia    1   0.02595 4.2276 -496.59\n- chesapeake  1   0.02747 4.2291 -496.54\n&lt;none&gt;                    4.2017 -495.48\n- wusa        1   0.10040 4.3021 -494.06\n- urals       1   0.12700 4.3287 -493.16\n- tornetrask  1   0.13070 4.3324 -493.04\n- jasper      1   0.26509 4.4668 -488.61\n- tasman      1   0.45420 4.6559 -482.60\n\nStep:  AIC=-497.43\nnhtemp ~ wusa + jasper + chesapeake + tornetrask + urals + mongolia + \n    tasman\n\n             Df Sum of Sq    RSS     AIC\n- mongolia    1   0.02433 4.2276 -498.59\n- chesapeake  1   0.03010 4.2334 -498.39\n&lt;none&gt;                    4.2033 -497.43\n- urals       1   0.12588 4.3292 -495.15\n- tornetrask  1   0.17416 4.3775 -493.54\n- wusa        1   0.17898 4.3823 -493.38\n- jasper      1   0.27028 4.4736 -490.39\n- tasman      1   0.46472 4.6680 -484.22\n\nStep:  AIC=-498.59\nnhtemp ~ wusa + jasper + chesapeake + tornetrask + urals + tasman\n\n             Df Sum of Sq    RSS     AIC\n&lt;none&gt;                    4.2276 -498.59\n- chesapeake  1   0.08207 4.3097 -497.80\n- urals       1   0.19293 4.4206 -494.12\n- tornetrask  1   0.24611 4.4737 -492.38\n- jasper      1   0.24694 4.4746 -492.36\n- wusa        1   0.30524 4.5329 -490.48\n- tasman      1   0.45557 4.6832 -485.75\n\n\n\nCall:\nlm(formula = nhtemp ~ wusa + jasper + chesapeake + tornetrask + \n    urals + tasman, data = globwarm1856)\n\nCoefficients:\n(Intercept)         wusa       jasper   chesapeake   tornetrask        urals  \n   -0.25497      0.09703     -0.21360     -0.04788      0.10893      0.21017  \n     tasman  \n    0.11268  \n\n\nThe step() function chooses the model with predictors wusa, jasper, chesapeake, tornatrask, urals, and tasman as the best model, eliminating westgreen and mongolia. Thus, observe the model chosen by stepwise elimination below.\n\n\n\nCall:\nlm(formula = nhtemp ~ wusa + jasper + chesapeake + tornetrask + \n    urals + tasman, data = globwarm1856)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41534 -0.10133  0.00671  0.09609  0.67544 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.25497    0.02149 -11.864  &lt; 2e-16 ***\nwusa         0.09703    0.03074   3.157 0.001961 ** \njasper      -0.21360    0.07523  -2.839 0.005207 ** \nchesapeake  -0.04788    0.02925  -1.637 0.103955    \ntornetrask   0.10893    0.03843   2.834 0.005281 ** \nurals        0.21017    0.08375   2.509 0.013246 *  \ntasman       0.11268    0.02922   3.856 0.000176 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.175 on 138 degrees of freedom\nMultiple R-squared:  0.4732,    Adjusted R-squared:  0.4503 \nF-statistic: 20.66 on 6 and 138 DF,  p-value: &lt; 2.2e-16\n\n\nNearly all variables are significant. The multiple \\(R^2\\) decreased from the full model, which makes sense, since \\(R^2\\) “rewards” the addition of more variables — \\(R^2=1-\\frac{RSS}{TSS}\\), and \\(RSS\\) can only increase upon the inclusion of more variables. However, the adjusted \\(R^2\\), which is defined as \\(R_a^2=1-\\frac{\\hat\\sigma_{model}^2}{\\hat\\sigma_{null}^2}\\), only increases if the inclusion of a variable increases the predictive value of the model. Thus, observing the adjusted \\(R^2\\) of both models, one sees that it is higher in the model selected by stepwise elimination."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#box-cox-transformation",
    "href": "projects/SLMMidtermExam.html#box-cox-transformation",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "4. Box-Cox Transformation",
    "text": "4. Box-Cox Transformation\nTo proceed building and evaluating models, it will be fruitful to evaluate the possible necessity for a transformation of the response variable, nhtemp. The method of evaluating possible transformations of the response is the Box-Cox transformation, which transforms a given response \\(y\\) to one of the form \\[g_\\lambda(y)=\\begin{cases}y^\\lambda & \\lambda \\not = 0 \\\\ \\log{y} & \\lambda=0\\end{cases}\\] where \\(\\lambda\\) is estimated by maximum likelihood. Thus, if the 95% confidence interval of \\(\\lambda\\) includes 1, it is most likely that no transformation is necessary; otherwise, the expected value of \\(\\lambda\\) should be used to transform the response.\nOne crucial aspect of Box-Cox transformation is that it requires the response to take only positive values. To check that this assumption is met, one can get a summary of the variable nhtemp in globwarm1856.\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6200 -0.2800 -0.1500 -0.1203  0.0300  0.6600 \n\n\nThe summary reveals that a majority of the nhtemp data is negative, which makes sense in terms of what the data is actually communicating: in layman’s terms, this means that a majority of days in the northern hemisphere are below freezing (0º C). Since the minimum value of nhtemp is small in terms of absolute value, one can amend the negative values by scaling values by 1, which will modify the intercept term in the linear regression model but none of the other coefficents (since the general structure of the data is preserved).\nNow there exists a variable nhtemp_pos which takes only positive values while maintaining the structure of the original response, nhtemp. This variable may thus be used instead of the original to test the need for a transformation via Box-Cox. The log-Likelihood function of \\(\\lambda\\) is plotted below, with the 95% confidence interval denoted with dashed lines.\n\n\n\n\n\n\n\n\n\nNotably, 1 is not within the 95% confidence interval for \\(\\lambda\\), meaning a transformation of the response is necessary. I have chosen \\(\\lambda=0.25\\), which seems close to the expected value of \\(\\lambda\\) (the middlemost vertical dashed line above).\nTo evaluate the efficacy of this transformation, one may build linear regression models that include the transformation and compare them to those that do not. Below are the models equivalent to the full model and stepwise elimination model introduced in the above subsection, with the transformation \\(g(y)=y^{0.25}\\) applied to the response. Though not included in the output here, I have also updated the previous models to assign nhtemp_pos as the response — these models will be discussed in the Conclusion section. We look first at the summary of the full model with the Box-Cox transformation applied.\n\n\n\nCall:\nlm(formula = I(nhtemp_pos^(0.25)) ~ wusa + jasper + westgreen + \n    chesapeake + tornetrask + urals + mongolia + tasman, data = globwarm1856)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.141734 -0.029997  0.001866  0.027110  0.147567 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.925782   0.007348 125.996  &lt; 2e-16 ***\nwusa         0.023906   0.011677   2.047 0.042558 *  \njasper      -0.048674   0.021247  -2.291 0.023507 *  \nwestgreen    0.004431   0.011382   0.389 0.697643    \nchesapeake  -0.007233   0.009263  -0.781 0.436242    \ntornetrask   0.029749   0.012255   2.427 0.016514 *  \nurals        0.036395   0.024871   1.463 0.145675    \nmongolia     0.007818   0.012457   0.628 0.531292    \ntasman       0.028598   0.008191   3.491 0.000648 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04781 on 136 degrees of freedom\nMultiple R-squared:  0.4805,    Adjusted R-squared:  0.4499 \nF-statistic: 15.72 on 8 and 136 DF,  p-value: 3.055e-16\n\n\nNotice above that 4 of the 8 predictors are significant, the multiple \\(R^2\\) has increased from the full model, the adjusted \\(R^2\\) has increased as well, and, most importantly, the residual standard error has decreased significantly. Now, we look at the summary of the stepwise elimination model with the Box-Cox transformation applied.\n\n\n\nCall:\nlm(formula = I(nhtemp_pos^(0.25)) ~ wusa + jasper + chesapeake + \n    tornetrask + urals + tasman, data = globwarm1856)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.144926 -0.029058  0.001823  0.026186  0.151504 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.922889   0.005838 158.097  &lt; 2e-16 ***\nwusa         0.028913   0.008350   3.463 0.000712 ***\njasper      -0.046562   0.020436  -2.278 0.024237 *  \nchesapeake  -0.010209   0.007946  -1.285 0.201036    \ntornetrask   0.033753   0.010439   3.233 0.001531 ** \nurals        0.039463   0.022749   1.735 0.085029 .  \ntasman       0.027687   0.007937   3.488 0.000653 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04754 on 138 degrees of freedom\nMultiple R-squared:  0.4787,    Adjusted R-squared:  0.4561 \nF-statistic: 21.12 on 6 and 138 DF,  p-value: &lt; 2.2e-16\n\n\nNotice above that 4 of the 6 predictors are significant, the multiple \\(R^2\\) has increased from the full model, the adjusted \\(R^2\\) has increased as well, and, most importantly, the residual standard error has decreased significantly. Overall, both Box-Cox transformed models perform better or equivalently to the pre-transformation models on all fronts."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#parameter-estimates-and-standard-errors",
    "href": "projects/SLMMidtermExam.html#parameter-estimates-and-standard-errors",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "5./6. Parameter Estimates and Standard Errors",
    "text": "5./6. Parameter Estimates and Standard Errors\nTo view and compare the paramter (i.e., coefficient) estimates and their standard errors, one may use the function compareCoefs() from the car package. We do so below. The first two columns represent the pre-transformation models (full and stepwise elimination), while the second two represent the post-transformation models.\n\n\nCalls:\n1: lm(formula = nhtemp ~ wusa + jasper + westgreen + chesapeake + \n  tornetrask + urals + mongolia + tasman, data = globwarm1856)\n2: lm(formula = nhtemp ~ wusa + jasper + chesapeake + tornetrask + urals + \n  tasman, data = globwarm1856)\n3: lm(formula = I(nhtemp_pos^(0.25)) ~ wusa + jasper + westgreen + \n  chesapeake + tornetrask + urals + mongolia + tasman, data = globwarm1856)\n4: lm(formula = I(nhtemp_pos^(0.25)) ~ wusa + jasper + chesapeake + \n  tornetrask + urals + tasman, data = globwarm1856)\n\n             Model 1  Model 2  Model 3  Model 4\n(Intercept) -0.24256 -0.25497  0.92578  0.92289\nSE           0.02701  0.02149  0.00735  0.00584\n                                               \nwusa         0.07738  0.09703  0.02391  0.02891\nSE           0.04293  0.03074  0.01168  0.00835\n                                               \njasper       -0.2288  -0.2136  -0.0487  -0.0466\nSE            0.0781   0.0752   0.0212   0.0204\n                                               \nwestgreen    0.00958           0.00443         \nSE           0.04184           0.01138         \n                                               \nchesapeake  -0.03211 -0.04788 -0.00723 -0.01021\nSE           0.03405  0.02925  0.00926  0.00795\n                                               \ntornetrask    0.0927   0.1089   0.0297   0.0338\nSE            0.0451   0.0384   0.0123   0.0104\n                                               \nurals         0.1854   0.2102   0.0364   0.0395\nSE            0.0914   0.0837   0.0249   0.0227\n                                               \nmongolia     0.04197           0.00782         \nSE           0.04579           0.01246         \n                                               \ntasman       0.11545  0.11268  0.02860  0.02769\nSE           0.03011  0.02922  0.00819  0.00794\n                                               \n\n\nThe greatest variation in parameter between the models occurs in (Intercept), which is to be expected since the Box-Cox evaluation required that the response be scaled by a constant. Otherwise, the parameters are either positive or negative across all four models, not a mix of the two. The parameter estimates of the non-transformed models tend to be more similar (in terms of absolute value) to each other than to the other models, and the same is true for the post-transformation models. Additionally, the standard error for each parameter decreases upon transformation of the response.\nWe can interpret the parameters contextually to provide further information about the nature of the model. To do so, let us look in particular at Model 1 above, the full model. In general, to interpret these parameters, one must imagine a world in which all variables except the one in question are held constant; for example, if one were to examine the parameter for mongolia, they would have to imagine that the trees in the western United States, Canada, Sweden, the Urals, and Tasmania did not grow, that the ice core in west Greenland did not melt or freeze further, and that the seashells found in Chesapeake Bay did not change either. Parameters may only be interpreted in isolation if other variables are held constant.\nThus, if none of the climate proxy information changed between two subsequent years, then the average yearly temperature in the Northern Hemisphere would be expected to decrease by 0.24256 ºC (this corresponds to the (Intercept) term).\nIf only the tree ring proxy information from Western USA changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the same direction by 0.07738 ºC.\nIf only the tree ring proxy information from Canada changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the opposite direction by 0.2288 ºC.\nIf only the ice core proxy information from west Greenland changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the same direction by 0.00958 ºC.\nIf only the seashell proxy information from Chesapeake Bay changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the opposite direction by 0.03211 ºC.\nIf only the tree ring proxy information from Sweden changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the same direction by 0.0927 ºC.\nIf only the tree ring proxy information from the Urals changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the same direction by 0.1854 ºC.\nIf only the tree ring proxy information from Mongolia changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the same direction by 0.04197 ºC.\nIf only the tree ring proxy information from Tasmania changed, then for every unit that it increased/decreased, the average yearly temperature would be expected to change in the same direction by 0.11545 ºC.\nA similar interpretation could be repeated for the other models above. However, these types of interpretations are not the most realistic for the given problem. In cases of experimental data, one has control over different factors and thus the “holding constant” of variables can have a tangible meaning. In cases of observational data, like globwarm, there is no such control over variables: it is impossible to stop a tree from growing for a year in the real world. Nevertheless, these interpretations do contribute to the understanding of the inner workings of the linear model, which can in turn give insights on the real world. For example, taking note of the fact that none of the parameters have an absolute value greater than 0.5, it can be stated that — in general — processes of climate change occur gradually, rather than rapidly."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#regression-diagnostics",
    "href": "projects/SLMMidtermExam.html#regression-diagnostics",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "7. Regression Diagnostics",
    "text": "7. Regression Diagnostics\nIn building the above models, some assumptions have been invoked without testing their validity. In this section, the goal is to test whether or not (1) the residuals of the models have constant variance, and (2) whether the residuals of the models are normally distributed. To do so, I have written the following function. The function first produces a plot of residuals against fitted values to evaluate (1) and a Q-Q plot to test (2). Then, the function tests (1) using the non-constant variance test and (2) using the Shapiro-Wilks’ test of normality.\nWe begin by evaluating the validity of the assumptions on the full model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 5.343177, Df = 1, p = 0.020804\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.97456, p-value = 0.00842\n\n\nThere seem to be areas of the Residuals vs. Fitted plot where the residuals bunch closer to zero (e.g. the interval [-0.4,-0.3]), and areas with higher variance (e.g. the interval [-0.1,0]). Furthermore, the rightmost tail of the Q-Q plot data trails far away from the line \\(y=x\\). This leads me to believe that neither of the assumptions are met, which is confirmed by the small \\(p\\)-values present in the nonconstant variance and Shapiro-Wilks’ tests. These small \\(p\\)-values indicate that the null hypotheses should be rejected, which correspond to homoskedasticity and normality, respectively. In other words, this model’s residuals are heterskedastic and non-normal.\nNow, we look at the stepwise elimination model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 3.755518, Df = 1, p = 0.052633\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.96745, p-value = 0.001601\n\n\nThere is less of an issue with “bunching” than in the previous model, with some instances of much greater variance (e.g. [-0.1,0]). The rightmost tail again trails far from the line \\(y=x\\), indicating non-normality. The nonconstant variance test returns a \\(p\\)-value just above 0.05, meaning the null is not rejected, but the Shapiro-Wilks’ test returns \\(p&lt;0.05\\), meaning the residuals are not normally distributed. Thus, the assumptions are not met by the stepwise elimination model either.\nNow we observe the full model with the response transformed via Box-Cox.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.003355372, Df = 1, p = 0.95381\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.98782, p-value = 0.2349\n\n\nLooking at the vertical axes of both plots, we note that they are on a notably smaller order of magnitude than the above two models’ were. Thus, though the shape of the Residuals vs. Fitted plot appears similar, the variance of the residuals is actually closer to constant. Similarly, the rightmost tail of the Q-Q plot does not trail nearly as much as in the two previous examples. Thus, it seems that the assumptions are met. This is confirmed by the two tests, both of which return \\(p&gt;0.05\\), meaning we fail to reject the null in both cases. Therefore, the Box-Cox transformation model with all predictors meets the assumptions.\nWe turn our attention now to the Box-Cox transformed model with predictors chosen by stepwise elimination.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.01402778, Df = 1, p = 0.90572\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.98568, p-value = 0.1378\n\n\nJust as above, the vertical axes are on a smaller order of magnitude than the first two models, so the variation in the Residuals vs. Fitted plot is smaller and the tail of the Q-Q plots travels less far away from the line \\(y=\\). The hypothesis tests again confirm that the assumptions are met.\nThus, overall, the Box-Cox transformed models meet the assumptions of linear regression, while the ones without the transformation do not."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#unusual-observations",
    "href": "projects/SLMMidtermExam.html#unusual-observations",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "8. Unusual Observations",
    "text": "8. Unusual Observations\nCertain observations can drastically affect the fit of a linear model. Such observations, termed “influential points,” may be found via a series of heuristics regarding the “distance” of an observation from its fitted value. I have written the following function to streamline finding influential points. First, it finds all points with large leverage, which are generally defined as those points which have a high impact in the fitting of a linear model. Heuristically, they are defined as those whose leverage are more than twice the mean of the hat values (recalling that the hat values are the diagonals of \\(H=X(X^TX)^{-1}X^T\\)). Then, it finds all outlier data, which are points that do not fit the model well, heuristically defined as those points whose standardized residuals are greater than 2 or less than -2. Finally, it checks all of these points to find the influential points, heuristically defined as those points whose Cook’s distance (\\(D_i=\\frac{1}{p}r_i^2\\frac{h_i}{1-h_i}\\), a function of both leverage and standardized residuals) is greater than \\(\\frac{4}{n}\\), where \\(n\\) is the sample size.\nTo observe the effect of influential points on the models, let us look at the model with the worst fit (the full model with not transformation) and the model with the best fit (the transformed model with predictors selected via stepwise transformation).\n\n\n[1] \"Large Leverage\"\nnamed numeric(0)\n[1] \"Outliers\"\n     1862      1878      1917      1918      1995      1997      1998      1999 \n-2.264011  2.253758 -2.570472 -2.038099  2.473404  2.652667  3.841535  2.746664 \n     2000 \n 2.446509 \n[1] \"Influential Points\"\n\n\n1862 1878 1917 1918 1995 1997 1998 1999 2000 \nTRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE \n\n\nThus, the influential points in the original model are the observations associated with the years 1862, 1878, 1917, 1918, 1995, 1997, 1998, 1999, and 2000. We check the same for the best model.\n\n\n[1] \"Large Leverage\"\nnamed numeric(0)\n[1] \"Outliers\"\n     1862      1878      1917      1995      1997      1998      1999      2000 \n-3.116175  2.280661 -2.723746  2.149998  2.423217  3.260177  2.571606  2.363861 \n[1] \"Influential Points\"\n\n\n 1862  1878  1917  1995  1997  1998  1999  2000 \n TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE \n\n\nThe influential points for the transformed stepwise elimination model are nearly the same as for the original, full model: 1862, 1878, 1917, 1995, 1997, 1998, 1999, and 2000. We thus see that choice of model does not have a great impact on the set of influential points. Furthermore, it is notable that the majority of influential points are from the last 10 years of the dataset; in other words, this inspection could be used as evidence that the environmental impact of modern technology is affecting global temperatures in profound ways.\nFor prediction purposes, we create a new dataset excluding the influential points.\nAdditionally, we refit the full model using this smaller dataset.\n\n\n\nCall:\nlm(formula = nhtemp_pos ~ wusa + jasper + westgreen + chesapeake + \n    tornetrask + urals + mongolia + tasman, data = globwarm1856_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26803 -0.08915 -0.00028  0.09221  0.32473 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.712301   0.021475  33.168  &lt; 2e-16 ***\nwusa         0.124087   0.033424   3.713 0.000306 ***\njasper      -0.111048   0.059732  -1.859 0.065328 .  \nwestgreen   -0.001757   0.031646  -0.056 0.955818    \nchesapeake  -0.074221   0.026616  -2.789 0.006109 ** \ntornetrask   0.192578   0.036680   5.250  6.2e-07 ***\nurals        0.026577   0.072313   0.368 0.713841    \nmongolia    -0.051917   0.036165  -1.436 0.153589    \ntasman       0.059068   0.024097   2.451 0.015594 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1301 on 127 degrees of freedom\nMultiple R-squared:  0.6041,    Adjusted R-squared:  0.5792 \nF-statistic: 24.23 on 8 and 127 DF,  p-value: &lt; 2.2e-16\n\n\nBoth the \\(R^2\\) and adjusted \\(R^2\\) have increased from the full model, and the residual standard error has decreased. The number of significant predictors is constant between the two. To further demonstrate the improvement this has caused, let us rerun the diagnostic analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 1.514217, Df = 1, p = 0.2185\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.99005, p-value = 0.444\n\n\nThe problems present earlier in the plots, as well as the small \\(p\\)-values in the nonconstant variance and Shapiro-Wilks’ tests, are all resolved. In other words, the removal of influential points allowed the original model to meet the assumptions that it previously failed to meet."
  },
  {
    "objectID": "projects/SLMMidtermExam.html#collinearity",
    "href": "projects/SLMMidtermExam.html#collinearity",
    "title": "Prediction of Global Temperatures with Climate Proxy Variables",
    "section": "9. Collinearity",
    "text": "9. Collinearity\nThough collinearity does not affect the predictive power of a linear regression model, it does affect the ability to discern effects of specific predictor variables on the response. Thus, though the main aim of this analysis is to predict values of nhtemp, it can also be useful to evaluate collinearity to understand the interaction between temperature and each of the climate proxies. In short, collinearity can cause instability of the parameter estimates \\(\\hat\\beta\\).\nCollinearity is the phenomenon in which there exists a (usually imperfect) linear relationship between some set of predictor variables. To determine the existence of collinearity, one may calculate the condition numbers \\(\\kappa_i\\) of the eigenvalues of the model matrix \\(X\\), which denote the relative size of the eigenvalues, or one may calculate the variance inflation factors (VIF) of the parameters of the model, which are large when the variance of its corresponding parameter, \\(\\textrm{var}\\hat\\beta_j\\), is large.\nI have written the following function to calculate and display the condition numbers of a given linear model.\nWe check the condition numbers of the full model as well as the stepwise elimination model (the “worst” and “best” models in terms of fit) in order to see if the removal of some predictors aided or worsened collinearity.\n\n\nEigenvalueCondition.Number467.7568171.00000076.1374422.47862652.5121532.98455944.4681883.24328722.8851934.52098211.2310256.4535754.7362469.9378613.19853712.093009\n\n\nEigenvalueCondition.Number303.8359951.00000063.6769242.18438252.1393062.41399838.8452972.7967295.5255627.4153453.4301169.411638\n\n\nLarge condition numbers are a sign of collinearity. Condition numbers are considered large when \\(\\kappa_i\\geq30\\). The largest condition number above, \\(\\kappa=12.093009\\) is less than 30, meaning the method of condition numbers does not indicate the presence of collinearity. Now, we check the variance inflation factors of the full model as well as the stepwise elimination model, in the same manner as above.\n\n\n      wusa     jasper  westgreen chesapeake tornetrask      urals   mongolia \n  5.229919   5.955474   1.782338   1.978483   4.696358   6.736267  10.187902 \n    tasman \n  1.635938 \n\n\n      wusa     jasper chesapeake tornetrask      urals     tasman \n  2.704498   5.572285   1.472545   3.446143   5.700140   1.553624 \n\n\nA variance inflation factor of 1 indicates completely orthogonal variables; the further a VIF is from 1, the greater an impact collinearity has on the instability of the model. In the larger model, there are some relatively large VIFs, namely, wusa, jasper, urals, and mongolia. The first three of these predictors’ VIFs decrease upon the removal of some predictors, while the final one is itself removed. To see the effect of the removal of some predictors, observe that the standard error for wusa in the full model is \\(\\sqrt{5.229919}=2.286902\\) times larger than it would be without collinearity, while in the smaller model, this is reduced to \\(\\sqrt{2.704498}=1.644536\\).\nOverall, none of the condition numbers or VIFs are large enough to indicate a concerning amount of collinearity."
  },
  {
    "objectID": "projects/csmlexam-mcmc.html",
    "href": "projects/csmlexam-mcmc.html",
    "title": "MCMC Sampling from a Multimodal Density",
    "section": "",
    "text": "Let \\(f(x) = k(\\exp[-3(x-2)]+\\exp[-(x-30)^2]+\\exp[-(x-20)^2/0.01])\\mathbf{1}_{\\{x\\geq 2\\}}\\) be the target density with normalising constant \\(k &gt;0\\) and support \\(x\\geq 2\\)."
  },
  {
    "objectID": "projects/csmlexam-mcmc.html#random-walk-metropolis-hastings",
    "href": "projects/csmlexam-mcmc.html#random-walk-metropolis-hastings",
    "title": "MCMC Sampling from a Multimodal Density",
    "section": "Random Walk Metropolis-Hastings",
    "text": "Random Walk Metropolis-Hastings\nFirst, I will sample from the target distribution using random walk Metropolis-Hastings (RWMH) with random noise \\(\\xi\\sim \\mathcal{N}(0,\\sigma^2)\\). The target density is clearly multimodal, with modes at \\(x=2,20,30\\). This means that small \\(\\sigma\\) will result in the the Markov chain failing to explore the entirety of the distribution, as it will gravitate towards the nearest mode from the initial chain value \\(X_0\\). Furthermore, the scaling factor of \\(1/0.01\\) for the mode at \\(X = 20\\) causes that mode to be a less ``weighty” attractor for the chain than the other modes. For a demonstration of these phenomena, see Figure 1, which generates \\(n=200,000\\) samples with the random noise’s standard deviation set to \\(\\sigma=0.5\\). Disregarding initial burn-in, the starting value of \\(X\\) determines the mode around which the chain explores. Furthermore, it is clear that the scaling factor of the mode at \\(X=20\\) greatly weakens its influence on the likelihood, as shown by the starting value \\(X_0=15\\) travelling to the mode at \\(X=2\\) instead of \\(X=20\\).\n\n\n\n\n\n\n\n\nFigure 1: Traceplots of four implementations of a random walk Metropolis-Hastings algorithm with normal noise. All implementations use \\(n=200,000\\) and \\(\\sigma=0.5\\).\n\n\n\n\n\nThis is clearly an issue. To create an effective sampler, the Markov chain must explore the all parts of the distribution, regardless of the initial value. In a multimodal target density such as this, that equates to mixing (or, convergence to the full stationary distribution), which is not present when \\(\\sigma=0.5\\).\nTo evaluate the sampler’s convergence to the stationary distribution I use the Gelman and Rubin diagnostic, \\(\\hat R\\), with 6 independent chains (i.e., twice the number of modes). Since \\(\\sigma=0.5\\) clearly was too small a standard deviation to allow full exploration, I repeated the process of creating chains and evaluating their \\(\\hat R\\) for integer values of \\(\\sigma\\geq 1\\) until the sampler seemed suitably near convergence. To generate \\(\\hat R\\), multiple chains are needed; I constructed 6 chains with start values \\(\\mathbf{X_0}=(2,20,30,2,20,30)\\) for each value of \\(\\sigma\\). The diagnostic plots are shown in Figure 2. From these iterations we can see that the distribution certainly converges when \\(\\sigma=6\\) (for some seeds it will converge for smaller \\(\\sigma\\), but this is not generalisable). To avoid unnecessary increased variance, I do not consider random walk samplers for values of \\(\\sigma &gt; 6\\).\n\n\n\n\n\n\n\n\nFigure 2: \\(\\hat R\\) diagnostic plots for the RWMH algorithm with \\(\\sigma\\) at varying levels.\n\n\n\n\n\nHowever, such a high variance comes at a cost: the mixing is very slow due to the high autocorrelation of the samples. As shown in Figure 3, it takes a lag several thousand samples for the autocorrelation to reach 0, which is a sizeable percentage of the number of generated samples. Thus, even though we have a sampler that converges to the stationary distribution, its mixing quality is exceptionally poor.\n\n\n\n\n\n\n\n\nFigure 3: Autocorrelation of a RWMH chain with \\(n=200,000\\), \\(X_0=15\\), and \\(\\sigma=6\\)."
  },
  {
    "objectID": "projects/csmlexam-mcmc.html#random-walk-metropolis-hastings-with-a-variable-transformation",
    "href": "projects/csmlexam-mcmc.html#random-walk-metropolis-hastings-with-a-variable-transformation",
    "title": "MCMC Sampling from a Multimodal Density",
    "section": "Random Walk Metropolis-Hastings with a Variable Transformation",
    "text": "Random Walk Metropolis-Hastings with a Variable Transformation\nThe support of \\(f(x)\\) is \\([2,\\infty)\\), but the random walk Metropolis-Hastings algorithm detailed above can propose values less than 2. Thus, to only propose values within the support of the target density, we need to apply variable transformation. I chose the transformation \\(y =h^{-1}(x)= \\log (x-2)\\), which is defined on \\([2,\\infty)\\). This results in \\(x=h(y)=e^y+2\\), with \\(|J_g(y)|=e^y\\). Therefore, we may sample from \\(g(y)\\) instead, for \\(g\\) defined as: \\[\ng(y) = f(e^y+2)e^y=k'e^y(\\exp[-3(e^y)]+\\exp[-(e^y-28)^2]+\\exp[-(e^y-18)^2/0.01])\n\\] where \\(k'&gt;0\\) is an unknown normalisation constant.\nWe may implement a random walk Metropolis-Hastings sampler for \\(g\\) much the same way we did for \\(f\\), being sure to transform the sampled values at the end to return values of \\(x\\), not \\(y\\). The sampler also needs to take an input value in terms of \\(y\\), not \\(x\\). I chose a start value of \\(Y_0=\\log(13)\\), equivalent to \\(X_0=15\\). I tested the algorithm for several values of \\(\\sigma\\) to see its convergence and mixing quality, similarly to the investigation in part (a), before deciding on \\(\\sigma=2\\), a much lower variance (and therefore faster mixing) but with similar convergence to \\(\\sigma=6\\) in (a) above.\n\n\n\n\n\n\n\n\nFigure 4: A traceplot for the variable-transformed RWMH sampler, run for \\(n=200,000\\) iterations with \\(\\sigma = 3\\) and \\(Y_0 = 0\\) (or, \\(X_0=2\\)).\n\n\n\n\n\nAs shown in Figure 4, the variable-transformed random walk Metropolis-Hastings converges to the target distribution."
  },
  {
    "objectID": "projects/csmlexam-mcmc.html#parallel-tempering",
    "href": "projects/csmlexam-mcmc.html#parallel-tempering",
    "title": "MCMC Sampling from a Multimodal Density",
    "section": "Parallel Tempering",
    "text": "Parallel Tempering\nWe will now use a parallel tempering algorithm to sample from \\(f\\). The algorithm I implemented has the structure shown in alg. 1, where \\(n\\) is the number of iterations, \\(\\mathbf{T} = (T_1, T_2,\\dots, T_M)\\) is the vector of temperatures, \\(\\sigma = (\\sigma_1,\\sigma_2,\\dots,\\sigma_M)\\) is the vector of standard deviations for each tempered chain, \\(\\mathbf{X_0}= (X_{0,1}, X_{0,2},\\dots,X_{0,M})\\) is the vector of initial values, and \\(X_{i,j}\\) denotes the \\(i\\)th sample of the \\(j\\)th tempered chain. I implemented the algorithm for \\(n=200,000\\) iterations on \\(M=5\\) chains with temperatures \\(\\mathbf{T} = (1, 5, 7, 10, 15)\\), standard deviations \\(\\sigma = (0.5, 0.7, 0.8, 0.9, 1)\\), and initial values \\(\\mathbf{X_0}=(15,15,15,15,15)\\). The transition kernels used for the ``\\(X\\)-step” are \\(N(X_{i-1,j}, \\sigma_j^2)\\), as stated in line 5 of alg. 1.\nThis algorithm will allow the chains to explore the distribution widely, while constraining the variance of the true sampling chain \\(T_1\\) to \\(0.5\\), greatly speeding up the mixing of the sampler. We see in Figure 5 that this increase in mixing quality does not come at the expense of convergence: the sampler is exploring the distribution fully and correctly.\n\nAlgorithm 1  \n\nAlgorithm 1: Parallel Tempering Algorithm\n\n\n\nProcedure ParallelTempering(\\[n, \\mathbf{T}, \\boldsymbol{\\sigma}^{2}, \\mathbf{X_0}\\])\n\n\n\n\nfor \\[i = 1, 2, \\dots, n\\] do\n\n\n\n\nProcedure X-Step()\n\n\n\n\nfor \\[j = 1, 2, \\dots, M\\] do\n\n\n\n\ngenerate \\[Y \\sim N(X_{i-1, j}, \\sigma_{j}^2)\\]\n\n\n\n\ngenerate \\[u \\sim U([0,1])\\]\n\n\n\n\nif \\[u \\leq \\left(\\frac{f(Y)}{f(X_{i-1,j})}\\right)^{\\frac{1}{T_j}}\\] then\n\n\n\n\n\\[X_{i,j} = Y\\]\n\n\n\n\nelse\n\n\n\n\n\\[X_{i,j} = X_{i-1,j}\\]\n\n\n\n\nend if\n\n\n\n\nend for\n\n\n\n\nend procedure\n\n\n\n\nProcedure Swap()\n\n\n\n\ngenerate \\[m =\\] random integer on \\[[1, M - 1]\\]\n\n\n\n\ngenerate \\[u' \\sim U([0,1])\\]\n\n\n\n\nif \\[u' \\leq \\frac{f(X_{i,m})^{1/T_{m+1}}f(X_{i,m+1})^{1/T_{m}}}{f(X_{i,m})^{1/T_m}f(X_{i,m+1})^{1/T_{m+1}}}\\] then\n\n\n\n\nexchange \\[X_{i,m}\\] and \\[X_{i,m+1}\\]\n\n\n\n\nend if\n\n\n\n\nend procedure\n\n\n\n\nend for\n\n\n\n\nend procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: A traceplot for the Parallel Tempering sampler, run for \\(n=200,000\\) iterations on \\(M=5\\) chains with \\(\\mathbf{T}=(1, 5, 7, 10, 15)\\), \\(\\sigma = (0.5, 0.7, 0.8, 0.9, 1)\\), and \\(\\mathbf{X_0}=(15,15,15,15,15)\\)."
  },
  {
    "objectID": "projects/csmlexam-mcmc.html#comparison-of-algorithms",
    "href": "projects/csmlexam-mcmc.html#comparison-of-algorithms",
    "title": "MCMC Sampling from a Multimodal Density",
    "section": "Comparison of Algorithms",
    "text": "Comparison of Algorithms\n\n\n\n\nTable 1: Comparison of Effective Sample Sizes (ESS) of the three algorithms, each run on \\(n=200,000\\) iterations.\n\n\n\n\n\n\n\nRWMH\nVariable-Transformed\nParallel Tempering\n\n\n\n\nESS (raw)\n129.6308641\n382.9275519\n3612.830967\n\n\nESS (%)\n0.0648154\n0.1914638\n1.806415\n\n\n\n\n\n\n\n\nThe algorithm in part (a) was able to reach convergence to the stationary distribution, but only with a very large variance in the random noise. This caused problematic levels of autocorrelation, indicating very slow mixing and thus poor mixing quality, as evidenced by its having the lowest effective sample size of the three algorithms (see Table 1). It is relatively computationally inexpensive, as it only requires one chain, but its low effective sample size means it would require a very high number of iterations to sample widely from the distribution.\nThe algorithm in part (b) was able to reach convergence at a lower, but still problematic, level of \\(\\sigma\\). We see in Figure 6 (a) that the sample autocorrelation function only takes a lag of a few hundred observations (as opposed to a few thousand in Figure 3) to decay to 0, but this is still quite large. The effect of this slower decay rate can be seen in the sampler’s effective sample size, shown in Table 1. Overall, though the mixing quality is better than the sampler from part (a), it is still not exceptional. However, this algorithm is the least computationally expensive, as it only proposes samples that are in the support of \\(f\\), automatically leading to a larger effective sample size than the sampler in part (a), and thus requiring fewer iterations to converge.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) truncated RWMH (part (b))\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) parallel tempering (part (c))\n\n\n\n\n\n\n\nFigure 6: Autocorrelation of the samples generated from the algorithms in parts (b) and (c).\n\n\n\n\nFinally, the algorithm constructed in part (c) wins in terms of convergence and mixing quality. It is able to explore the entire distribution well, as shown in Figure 5. Furthermore, we can see in Figure 6 (b) that the autocorrelation times are largely reduced compared to the algorithms from (a) and (b). However, it should be noted that the autocorrelation function never fully approaches 0, but rather converges to a positive value quickly. Nevertheless, the parallel tempering algorithm still has mixing quality, corresponding to a much larger effective sample size, shown in Table 1. The downside of this algorithm is that it is by far the most computationally expensive, seeing as it requires 5 parallel chains instead of 1."
  },
  {
    "objectID": "projects/csmlexam-mcmc.html#confidence-intervals-using-mcmc-samplers",
    "href": "projects/csmlexam-mcmc.html#confidence-intervals-using-mcmc-samplers",
    "title": "MCMC Sampling from a Multimodal Density",
    "section": "Confidence Intervals using MCMC Samplers",
    "text": "Confidence Intervals using MCMC Samplers\nLet us now estimate \\(\\mathbb{E}(X)\\) where \\(X\\) is a random variable with density \\(f\\). Let \\(\\hat I^{(a)}_n, \\hat I^{(b)}_n, \\hat I^{(c)}_n\\) denote the estimate of \\(\\mathbb{E}(X)\\) generated by the MCMC algorithms detailed in parts (a), (b), and (c), respectively, run for \\(n\\) iterations.\nWe would like to find 95% confidence intervals for each \\(\\hat I_n\\), at varying values of \\(n\\). I generated confidence intervals for \\(1,000\\leq n\\leq 100,000\\), increasing \\(n\\) by \\(100\\) on each iteration. Thus, we have \\(k=1000\\) batches with a batch size of \\(m=100\\). We note that these confidence intervals take the form \\[\n\\hat I_n \\pm t_{1-\\delta/2}\\frac{s_m(n)}{\\sqrt{n}} = \\hat I_n \\pm t_{0.975}\\sqrt{\\frac{\\frac{1}{99}\\sum_{k=1}^{1000}(\\hat I_{k,n}-\\hat I_n)^2}{1000}}\n\\] where \\(\\hat I_{k,n}=\\frac{100}{n}\\sum_{i=999n/100}^{1000n/100}X_i\\) is the \\(k\\)th batch mean and \\(t_{1-\\delta/2}\\) denotes the quantile of \\(1-\\delta/2\\) of a Student’s \\(t\\)-distribution with \\(m-1=99\\) degrees of freedom. To generate the plots of these CIs for increasing \\(n\\), however, I did not calculate each upper and lower bound by hand. I used the mcse function from the mcmcse package and qt from base R. The results are included in Figure 7.\n\n\n\n\n\n\n\n\nFigure 7: Estimates of \\(\\mathbb E(X)\\) for chains of length \\(n\\in \\{100,200,\\dots, 200,000\\}\\) generated by the algorithms built in parts (a), (b), and (c). The gray shading denotes the 95% confidence intervals for \\(\\hat I_n\\). The value of \\(\\mathbb{E}(X)\\) found via numerical integration is included as the dashed line.\n\n\n\n\n\nThe plots in Figure 7 are somewhat dependent on seed, but their behaviour is generalisable. The RWMH algorithm only consistently approaches the true value of \\(\\mathbb E(X)\\) for large \\(n\\), either over- or under-estimating this value beforehand. The variable-transformed and parallel tempering algorithms both converge near the true value of \\(\\mathbb E (X)\\) for smaller \\(n\\). Between the latter two, the variable-transformed algorithm tends to converge faster than parallel tempering, but has wider confidence intervals. The parallel tempering has the smallest confidence intervals, but these do not always include the true value of \\(\\mathbb E(X)\\). Thus, for confidence intervals, I would choose the variable-transformed algorithm over the other two."
  },
  {
    "objectID": "projects/csmlexam-mcmc.html#sec-code",
    "href": "projects/csmlexam-mcmc.html#sec-code",
    "title": "MCMC Sampling from a Multimodal Density",
    "section": "Code Appendix",
    "text": "Code Appendix\n\n# Document setup\nknitr::opts_chunk$set(\n  echo = FALSE,\n  collapse = TRUE,\n  comment = \"#&gt;\",\n  dev = \"ragg_png\"\n)\nlibrary(tidyverse)\nlibrary(kableExtra)\n# Question 1\n# part (a)\n\n# target:\nf &lt;- function(x){\n  (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-(x-20)^2/.01)) * as.numeric(x&gt;=2)\n}\n\n# random walk:\nrwproposal &lt;- function(n, sigma, X0){\n  Xres &lt;- rep(NA, n)\n  X &lt;- X0\n  for (i in 1:n){\n    Y &lt;- X + rnorm(1, mean = 0, sd = sigma)\n    if (runif(1) &lt;= f(Y)/f(X)){\n      X &lt;- Y\n    }\n    Xres[i] &lt;- X\n  }\n  return(Xres)\n}\nsample_size &lt;- 2e5\n\nX5 &lt;- rwproposal(n = sample_size, sigma = 0.5, X0 = 5)\nX15 &lt;- rwproposal(n = sample_size, sigma = 0.5, X0 = 15)\nX20 &lt;- rwproposal(n = sample_size, sigma = 0.5, X0 = 20)\nX25 &lt;- rwproposal(n = sample_size, sigma = 0.5, X0 = 25)\n\nXvar &lt;- data.frame(X5 = X5, X15 = X15, X20 = X20, X25 = X25,\n                   index = 1:sample_size) %&gt;%\n  pivot_longer(cols = starts_with(\"X\"),\n               names_to = \"startval\",\n               names_prefix = \"X\",\n               values_to = \"X\") %&gt;%\n  mutate(startval = fct_relevel(startval, c(\"25\",\"20\",\"15\",\"5\")))\n\nggplot(Xvar, aes(x = index, y = X, color = startval)) +\n  geom_line(linewidth = 0.5) +\n  theme_minimal() +\n  labs(x = \"Index\", y = \"X\", color = \"Initial X value\") +\n  theme(text = element_text(family = \"Times New Roman\"))\nlibrary(coda)\nX0s &lt;- c(2,20,30,2,20,30)\nchains1 &lt;- lapply(X0s, \n    function(x0) mcmc(rwproposal(n = sample_size, sigma = 1, X0 = x0)))\nchains2 &lt;- lapply(X0s, \n    function(x0) mcmc(rwproposal(n = sample_size, sigma = 2, X0 = x0)))\nchains3 &lt;- lapply(X0s, \n    function(x0) mcmc(rwproposal(n = sample_size, sigma = 3, X0 = x0)))\nchains4 &lt;- lapply(X0s, \n    function(x0) mcmc(rwproposal(n = sample_size, sigma = 4, X0 = x0)))\nchains5 &lt;- lapply(X0s, \n    function(x0) mcmc(rwproposal(n = sample_size, sigma = 5, X0 = x0)))\nchains6 &lt;- lapply(X0s, \n    function(x0) mcmc(rwproposal(n = sample_size, sigma = 6, X0 = x0)))\n\nchains1 &lt;- mcmc.list(chains1)\nchains2 &lt;- mcmc.list(chains2)\nchains3 &lt;- mcmc.list(chains3)\nchains4 &lt;- mcmc.list(chains4)\nchains5 &lt;- mcmc.list(chains5)\nchains6 &lt;- mcmc.list(chains6)\n\nop &lt;- par(no.readonly = TRUE)\npar(mfrow = c(3, 2), mar = c(3, 3, 2, 1), oma = c(0, 0, 2, 0))\n\ngelman.plot(chains1, auto.layout = FALSE, main = expression(sigma == 1))\ngelman.plot(chains2, auto.layout = FALSE, main = expression(sigma == 2))\ngelman.plot(chains3, auto.layout = FALSE, main = expression(sigma == 3))\ngelman.plot(chains4, auto.layout = FALSE, main = expression(sigma == 4))\ngelman.plot(chains5, auto.layout = FALSE, main = expression(sigma == 5))\ngelman.plot(chains6, auto.layout = FALSE, main = expression(sigma == 6))\nstandard_mcmc_sample &lt;- rwproposal(n = sample_size, sigma = 6, X0 = 15)\nacf(standard_mcmc_sample, type = \"correlation\",\n    lag.max = 10000, main = \"\")\n# part (b)\nftrans &lt;- function(y) f(exp(y)+2)*exp(y)\n\n# random walk:\nrwtransform &lt;- function(n, sigma, Y0){\n  Ysamp &lt;- rep(NA, n+1)\n  Ysamp[1] &lt;- Y0\n  Y &lt;- Y0\n  for (i in 1:n){\n    Yproposal &lt;- Y + rnorm(1, mean = 0, sd = sigma)\n    if (runif(1) &lt;= ftrans(Yproposal)/ftrans(Y)){\n      Y &lt;- Yproposal\n    }\n    Ysamp[i+1] &lt;- Y\n  }\n  Xsamp &lt;- 2 + exp(Ysamp)\n  return(Xsamp)\n}\n\n# results:\n# need a starting Y value not X value!\nxtransform &lt;- rwtransform(sample_size, sigma = 2, Y0 = log(13))\ntrceplt &lt;- data.frame(index = 1:length(xtransform),\n                      X = xtransform)\nggplot(trceplt, aes(x = index, y = X)) +\n  geom_line(linewidth = 0.3) +\n  theme_minimal() +\n  theme(text = element_text(family = \"Times New Roman\"))\n# part (c)\n\nparalleltemp &lt;- function(iterations, Ts, sigmas, X0) {\n\n  M &lt;- length(Ts)\n  X &lt;- matrix(nrow=iterations+1, ncol = M)\n  X[1,] &lt;- X0\n\n  for (n in 1:iterations) {\n    \n    # we are going to do a x-step; use log-domain for underflow issues\n    for (i in 1:M){\n      X[n+1, i] &lt;- X[n, i]\n      Y &lt;- rnorm(1, X[n,i], sigmas[i])\n      alpha &lt;- exp((1/Ts[i])*(log(f(Y))-log(f(X[n,i]))))\n      if (!is.na(alpha) && runif(1) &lt;= alpha){\n        X[n+1, i] &lt;- Y\n      }\n    }\n\n    # we are going for a swap\n    m &lt;- sample(1:(M-1), 1)\n    num &lt;- (f(X[n+1,m])^(1/Ts[m+1])*f(X[n+1,m+1])^(1/Ts[m]))\n    denom &lt;- (f(X[n+1,m])^(1/Ts[m])*f(X[n+1,m+1])^(1/Ts[m+1]))\n    # log1 &lt;- (1/Ts[m+1])*log(f(X[n+1,m]))\n    if (runif(1) &lt;= num/denom){\n      store &lt;- X[n+1,m+1]\n      X[n+1,m+1] &lt;- X[n+1,m]\n      X[n+1,m] &lt;- store\n    }\n  }\n  return(X)\n}\n\nTs &lt;- c(1, 5, 7, 10, 15)\nsigmas &lt;- c(0.5,0.7,0.8,0.9,1)\nX0s &lt;- c(15,15,15,15,15)\nsamples &lt;- paralleltemp(sample_size, Ts, sigmas, X0 = X0s)\nparallel_sample &lt;- data.frame(sample = samples[,1],\n                               index = 1:length(samples[,1]))\nggplot(parallel_sample, aes(x = index, y = sample)) +\n  geom_line(linewidth = 0.02) +\n  theme_minimal() +\n  theme(text = element_text(family = \"Times New Roman\"))\ness &lt;- data.frame(a = effectiveSize(mcmc(standard_mcmc_sample)),\n                        b = effectiveSize(mcmc(xtransform)),\n                        c = effectiveSize(mcmc(samples[,1]))) %&gt;%\n  add_row(a = effectiveSize(mcmc(standard_mcmc_sample))/sample_size*100,\n          b = effectiveSize(mcmc(xtransform))/sample_size*100,\n          c = effectiveSize(mcmc(samples[,1]))/sample_size*100)\nrownames(ess) &lt;- c(\"ESS (raw)\", \"ESS (%)\")\nknitr::kable(ess,\n             col.names = c(\"RWMH\",\n                           \"Variable-Transformed\",\n                           \"Parallel Tempering\"))\nacf(xtransform, type = \"correlation\", lag.max = 1000, main = \"\")\nacf(samples[,1], type = \"correlation\", lag.max = 1000, main = \"\")\nlibrary(mcmcse)\nlibrary(data.table)\n\na &lt;- rwproposal(sample_size, sigma = 6, X0 = 15)\nb &lt;- rwtransform(sample_size, sigma = 2, Y0 = log(13))\nc &lt;- paralleltemp(sample_size, Ts, sigmas, X0s)[,1]\n\nN &lt;- seq(1000, 100000, by = 100)\nconfints_list &lt;- vector(\"list\", length = length(N))\nfor (j in seq_along(N)){\n  i &lt;- N[j]\n  \n  asamp &lt;- a[1:i]\n  bsamp &lt;- b[1:i]\n  csamp &lt;- c[1:i]\n  \n  ase &lt;- mcse(asamp, size = 100)\n  bse &lt;- mcse(bsamp, size = 100)\n  cse &lt;- mcse(csamp, size = 100)\n  \n  ta &lt;- qt(0.975, df = 99)\n  tb &lt;- qt(0.975, df = 99)\n  tc &lt;- qt(0.975, df = 99)\n\n  df &lt;- data.frame(n = rep(i, 3),\n                     alg = c(\"a\", \"b\", \"c\"),\n                     lower = c(ase$est - ta*ase$se,\n                               bse$est - tb*bse$se,\n                               cse$est - tc*cse$se),\n                     est = c(ase$est, bse$est, cse$est),\n                     upper = c(ase$est + ta*ase$se,\n                               bse$est + tb*bse$se,\n                               cse$est + tc*cse$se))\n  confints_list[[j]] &lt;- df\n}\n\nconfints &lt;- rbindlist(confints_list)\nnormalise &lt;- integrate(function(x) f(x), lower = 2, upper = Inf)\nexpectation &lt;- integrate(function(x) 1/normalise$value*f(x)*x, lower = 2,\n                         upper = Inf)\n\nlibrary(patchwork)\na1 &lt;- ggplot(subset(confints, alg == \"a\"), aes(x = n, y = est)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  geom_line() +\n  geom_hline(yintercept = expectation$value, linetype = \"dashed\",\n             color = \"navy\") + \n  scale_y_continuous(breaks = sort(round(c(10,20,30, expectation$value),\n                                         digits = 2))) +\n  theme_minimal() +\n  labs(x = expression(n),\n       y = expression(hat(I[n])^{(a)})) +\n  theme(text = element_text(family = \"Times New Roman\"),\n        axis.title.y = element_text(angle = 90))\n\na2 &lt;- ggplot(subset(confints, alg == \"b\"), aes(x = n, y = est)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  geom_line() +\n  geom_hline(yintercept = expectation$value, linetype = \"dashed\",\n             color = \"navy\") + \n  scale_y_continuous(breaks = sort(round(c(10,20,30, expectation$value),\n                                         digits = 2))) +\n  theme_minimal() +\n  labs(x = expression(n),\n       y = expression(hat(I[n])^{(b)})) +\n  theme(text = element_text(family = \"Times New Roman\"),\n        axis.title.y = element_text(angle = 90))\n\na3 &lt;- ggplot(subset(confints, alg == \"c\"), aes(x = n, y = est)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3) +\n  geom_line() +\n  geom_hline(yintercept = expectation$value, linetype = \"dashed\",\n             color = \"navy\") + \n  scale_y_continuous(breaks = sort(round(c(10,20,30, expectation$value),\n                                         digits = 2))) +\n  theme_minimal() +\n  labs(x = expression(n),\n       y = expression(hat(I[n])^{(c)})) +\n  theme(text = element_text(family = \"Times New Roman\"),\n        axis.title.y = element_text(angle = 90))\n\na1 / a2 / a3"
  },
  {
    "objectID": "projects/thesis.html",
    "href": "projects/thesis.html",
    "title": "The Legacy of Eugenics in Statistics Pedagogy (Senior Thesis in Mathematics)",
    "section": "",
    "text": "My senior thesis in mathematics, advised by Jemma Lorenat and Shahriar Shahriari, is titled “Regression to the Mean”: The Confluence of Eugenics and Statistics in the 19th and 20th Centuries. My research for this project was funded and supported by the Humanities Studio Undergraduate Fellowship. Furthermore, my thesis earned distinction from the Department of Mathematics and Statistics at Pomona College. I have included my data analysis and visualization chapter below. The full thesis is available through an open-access license here.\nAbstract: The work of this thesis is twofold — first, qualitatively characterizing the confluence between the British eugenics and statistics movements in the late 19th and early 20th centuries, and second, quantitatively analyzing the effect of this foundation on pedagogical materials in the growing field of statistics between 1880 and 1970. Towards the first goal, the history of the method of least squares, state statistics, and positive and negative eugenics are outlined, followed by a close reading of the foundational texts authored by Francis Galton and Karl Pearson that introduced linear regression. Towards the latter goal, English-language statistics textbooks published between 1880 and 1970 are analyzed using text-mining algorithms to establish the extent of their co-existence with hereditary and eugenic studies.\nDownload PDF file."
  },
  {
    "objectID": "projects/SLMFinal.html#exploratory-analysis",
    "href": "projects/SLMFinal.html#exploratory-analysis",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nTo better understand our data, we began with an exploratory analysis. We first observe that the dataset is a time series. Thus, to avoid possible autocorrelations of variables, we have limited our observations for analysis to the year 2015. This year has 179 observations, each corresponding to a different country. To test the predictive capability within the same year, we have reserved 20% of the data (every 5th observation) for use as a test dataset, while the remaining observations will be used to train the models.\nWe first plotted the raw relationship between GDP per capita and life expectancy in 2015. Though we removed 20% of the data to use as test cases, the overall shape of the data does not change. In this plot, we observe a positive relationship between GDP per capita and Life Expectancy, and this relationship appears non-linear. Most likely, it is logarithmic. We will test the assumptions on an ordinary least squares regression and inform our analysis from there.\nFurthermore, we have colored each datapoint by region to show general geographical trends, which reflect the histories of different regions, including how wars and exploitation of regions in the Global South have impacted economies. Most interesting about this spread is that we see high levels of within-group variation. For example, the datapoints for Africa have near 0 GDP per capita, but their life expectancies range across the vertical axis. Conversely, the datapoints for the European Union have life expectancies near 80, but GDP per capita that range across the horizontal axis. We will analyze this phenomenon further using Analysis of Variance (ANOVA) in a later section.\n\n\n\n\n\n\n\n\n\nFollowing this initial graph, we would like to see other relationships between variables in the dataset. We look at a years of schooling vs. GDP per capita plot to understand if other similar relationships exist between GDP per capita and status metrics.\n\n\n\n\n\n\n\n\n\nWe see a similar nonlinear (likely logarithmic) trend in the plot of years of schooling against GDP per capita. This plot also contains the same within-group variation phenomenon as above. Thus, we expect that there is likely a linear relationship between life expectancy and years of schooling. We observe this relationship below.\n\n\n\n\n\n\n\n\n\nWe do see a linear relationship, with a low signal-to-noise. Thus, while a model predicting life expectancy with only GDP per capita would likely require a transformation of the response, a model predicting life expectancy with only years of schooling would likely not need such a transformation.\nTo finish our exploratory data analysis, we note that there is a high likelihood of collinearity between predictor variables in this dataset. For example, the predictors related to rates of immunization coverage are likely collinear due to the fact that several immunizations may given to a child in one sitting. We investigate one such possible collinearity in the plots below, which show every possible combination of the four vaccine coverage variables (percent coverage of Hepatitis B vaccination, percent coverage of Measles vaccination, percent coverage of Polio vaccination, percent coverage of Diphtheria vaccination)\n\n\n\n\n\n\n\n\n\nWhile some plots have a clearer linear relationship than others, and there is definitely a ceiling effect occurring, there seems to be some degree of linearity present between each pair of immunizations. We will investigate collinearity more rigorously in a later section."
  },
  {
    "objectID": "projects/SLMFinal.html#preliminary-models",
    "href": "projects/SLMFinal.html#preliminary-models",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Preliminary Models",
    "text": "Preliminary Models\nWe begin our data analysis with a multiple linear regression model. We use all possible predictors to model life expectancy, and fit the model using the ordinary method of least squares. For conciseness, we will refer to this model as the Full OLS model. Below is displayed a summary of the Full OLS model.\n\n\n\nCall:\nlm(formula = Life_expectancy ~ Region + Infant_deaths + Under_five_deaths + \n    Adult_mortality + Alcohol_consumption + Hepatitis_B + Measles + \n    BMI + Polio + Diphtheria + Incidents_HIV + GDP_per_capita + \n    Population_mln + Thinness_ten_nineteen_years + Thinness_five_nine_years + \n    Schooling + Economy_status_Developing, data = train_lifeexp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3525 -0.8608  0.0120  0.7495  3.2817 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          8.696e+01  2.702e+00  32.189  &lt; 2e-16 ***\nRegionAsia                           2.434e-01  4.780e-01   0.509  0.61165    \nRegionCentral America and Caribbean  1.451e+00  5.163e-01   2.809  0.00581 ** \nRegionEuropean Union                -7.813e-01  7.436e-01  -1.051  0.29554    \nRegionMiddle East                    4.371e-01  5.918e-01   0.739  0.46161    \nRegionNorth America                  1.701e-01  1.132e+00   0.150  0.88085    \nRegionOceania                       -1.322e+00  6.436e-01  -2.054  0.04213 *  \nRegionRest of Europe                 2.680e-01  5.926e-01   0.452  0.65195    \nRegionSouth America                  1.810e+00  5.747e-01   3.149  0.00207 ** \nInfant_deaths                       -6.433e-02  4.785e-02  -1.344  0.18135    \nUnder_five_deaths                   -4.827e-02  3.123e-02  -1.546  0.12486    \nAdult_mortality                     -4.791e-02  3.286e-03 -14.581  &lt; 2e-16 ***\nAlcohol_consumption                 -2.773e-02  5.413e-02  -0.512  0.60942    \nHepatitis_B                         -1.463e-02  2.710e-02  -0.540  0.59039    \nMeasles                              9.839e-03  9.000e-03   1.093  0.27651    \nBMI                                 -1.195e-01  9.229e-02  -1.295  0.19789    \nPolio                                3.358e-03  2.556e-02   0.131  0.89570    \nDiphtheria                           1.380e-02  3.092e-02   0.446  0.65625    \nIncidents_HIV                        2.013e-01  9.216e-02   2.184  0.03093 *  \nGDP_per_capita                       2.619e-05  9.277e-06   2.824  0.00557 ** \nPopulation_mln                      -3.914e-05  7.186e-04  -0.054  0.95666    \nThinness_ten_nineteen_years         -2.794e-01  1.571e-01  -1.779  0.07785 .  \nThinness_five_nine_years             2.392e-01  1.568e-01   1.525  0.12990    \nSchooling                           -4.204e-02  8.480e-02  -0.496  0.62104    \nEconomy_status_Developing           -3.129e+00  7.253e-01  -4.314 3.33e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.225 on 119 degrees of freedom\nMultiple R-squared:  0.9798,    Adjusted R-squared:  0.9758 \nF-statistic: 240.8 on 24 and 119 DF,  p-value: &lt; 2.2e-16\n\n\nThere are a few key takeaways from this summary. First, the standard errors are very small, with the largest being 2.702 (associated with the (Intercept) term). This indicates that the fitted linear relationship is a very good approximation of the actual relationship between the datapoints, which is corroborated by the very high value of \\(R^2=0.9798\\). However, this excellent fit is accompanied by only a few significant predictors: RegionCentral America and Caribbean, RegionOceania, RegionSouth America, Adult_mortality, Incidents_HIV, GDP_per_capita, and Economy_status_Developing (out of 24 total predictors). This is a definite symptom of collinearity. We will address collinearity more fully in a later section.\nIn regards to the first goal of this analysis, we see that, while GDP per capita is a significant predictor of life expectancy, it is not the only significant predictor. We thus compare the above model to the simple linear regression model with life expectancy as the response and GDP per capita as the sole predictor. First, let us observe a summary of the simple linear regression model.\n\n\n\nCall:\nlm(formula = Life_expectancy ~ GDP_per_capita, data = train_lifeexp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.585  -3.296   1.542   4.465   8.672 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6.828e+01  6.355e-01 107.445   &lt;2e-16 ***\nGDP_per_capita 2.648e-04  2.743e-05   9.653   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.133 on 142 degrees of freedom\nMultiple R-squared:  0.3962,    Adjusted R-squared:  0.3919 \nF-statistic: 93.18 on 1 and 142 DF,  p-value: &lt; 2.2e-16\n\n\nWhile the standard errors are still small and the predictor significant, the \\(R^2\\) value has decreased greatly to 0.3962. Since the predictor of the simple linear regression (SLR) model is a subset of predictors of the Full OLS model, we may use the nested model hypothesis test to evaluate the significance of the difference between the two models. The nested model hypothesis test for our current models is as follows: \\[\\begin{cases} H_0:& \\textrm{the SLR and Full OLS models have equivalent levels of accuracy} \\\\ H_a:& \\textrm{the larger Full OLS model is more accurate than the SLR model} \\end{cases}\\]\n\n\n\n\n\nRes.DfRSSDfSum of SqFPr(&gt;F)\n\n\n\n119178                     \n\n1425.34e+03-23-5.16e+031509.82e-77\n\n\n\n\nSince the \\(p\\)-value is less than 0.05, we reject the null hypothesis. Therefore, without any transformation or other considerations like model diagnostics, the Full OLS model is less accurate than the SLR model."
  },
  {
    "objectID": "projects/SLMFinal.html#model-diagnostics",
    "href": "projects/SLMFinal.html#model-diagnostics",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\nAn important consideration in building linear regression models using ordinary least squares is the adherence of the model to certain assumptions. In particular, one must assure that the residuals (the distance between the fitted values and actual datapoints) have constant variance — also called homoskedasticity — and that the distribution of the residuals is normal. To evaluate these assumptions, we have written a function, diagnostics(), which takes a linear model as input and returns two plots and two tests to assess the assumptions. The first plot, fitted vs. residuals, gives a visual representation of the variance of the residuals, which can be used to understand if the variance is constant. We include the line \\(y=0\\), towards which the residuals are minimized, to observe if they are evenly and randomly distributed on either side of the line. This plot corresponds with the output of the non-constant variance test. The second plot, the Normal Q-Q plot, displays the sample vs. theoretical quantiles of the residuals. If these quantiles are equal, then the residuals will be perfectly normal; thus, in this plot, we look for adherence to the line \\(y=x\\). This plot corresponds with the output of the Shapiro-Wilks test of normality.\nBelow we see the output of the diagnostics() function for the Full OLS model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 4.692216, Df = 1, p = 0.0303\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.98961, p-value = 0.3632\n\n\nThe Full OLS model breaks the homoskedasticity assumption. This can be seen in the leftmost tail of the residuals vs. fitted plot above: on the interval of fitted values [50, 57], nearly all of the residuals are positive, so even though the rest of the plot seems randomly distributed across positive and negative, the overall variance cannot be constant. We confirm this result by the \\(p\\)-value of the non-constant variance test, \\(p=0.0303&lt;0.05\\), which allows us to reject the null hypothesis of homoskedasticity.\nSince the variance is non-constant, the residuals cannot be truly normally distributed. However, the Q-Q plot above shows a good adherence to the line and the Shapiro-Wilks test returns a \\(p\\)-value of \\(p=0.3632&gt;0.05\\), meaning we fail to reject the null hypothesis of normality. Thus, though the residuals are heteroskedastic, we may view them as approximately normally distributed.\nWe can also view the diagnostic output for the SLR model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 1.433745, Df = 1, p = 0.23115\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.9175, p-value = 2.36e-07\n\n\nThe most egregious violation of the assumptions above is the violation of normality. With \\(p=2.36e-07\\) in the Shapiro-Wilks test, we can soundly reject the null hypothesis of normality, corroborated visually by the non-equivalence of the quantiles in the Q-Q plot.\nIf normality is assumed, however, the variance is constant: there is an equivalent proportion of negative and positive residuals. The nonconstant variance test \\(p\\)-value, \\(p=0.23115\\), agrees with this statement. However, since the residuals are clearly not randomly distributed, homoskedasticity is a bit of a moot point.\nThus, in terms of assumptions, the Full OLS model once again trumps the SLR model. In the following sections, further models will be built to attempt to resolve the Full OLS model’s violation of assumptions."
  },
  {
    "objectID": "projects/SLMFinal.html#influential-points",
    "href": "projects/SLMFinal.html#influential-points",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Influential Points",
    "text": "Influential Points\nIn order to apply transformations to the response variable, assess collinearity, and later on run shrinkage methods, one should first analyze the presence and location of influential points in the dataset. Influential points can drastically change the fit of a model. Box-Cox transformations and shrinkage methods are sensitive to influential points, and thus their removal can give a more accurate treatment of the data.\nSimilarly to the above section, we wrote a function, unusual_obs(), which will identify all points of high leverage, outlier points, and influential points. A point with high leverage is defined as one whose leverage is twice the mean leverage in a given dataset. In turn, leverage is given as the diagonals of the hat matrix \\(H=X(X^TX)^{-1}X^T\\). Points with high leverage may potentially have a big impact on the fit of the model, but only if they are outliers as well. Outliers are points that do not fit the model well. To find outliers, we locate all datapoints whose standardized residuals are greater than 2 or less than \\(-2\\). Finally, we find the influential points by calculating the Cook’s distance of each point identified as either a point of high leverage or an outlier. If the Cook’s distance \\(D_i\\) of any of these points is greater than \\(\\frac{4}{n}\\) (\\(n\\) being the sample size), then we heuristically consider those points influential.\nBelow we identify the outliers for the Full OLS model.\n\n\n[1] \"Large Leverage\"\n        9        15        23        25        31        33        37        68 \n0.5297997 0.3730873 0.3950965 0.6157479 0.6516889 0.6157479 0.6404849 0.4419994 \n      106       128 \n0.3872335 0.9594019 \n[1] \"Outliers\"\n        9        14        46        53       107 \n-2.513456 -2.032598  3.073952  2.291787  2.029910 \n[1] \"Influential Points\"\n    9    15    23    25    31    33    37    68   106   128 \n TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE \n    9    14    46    53   107 \n TRUE FALSE  TRUE  TRUE  TRUE \n\n\nThus, we have influential points at observations 9, 31, 128 (which have high leverage), and 46, 53, 107 (which are outliers). Let’s see which points these are.\n\n\nCountryRegionLife_expectancyGDP_per_capitaEswatiniAfrica55.43680ChinaAsia75.98016PhilippinesAsia70.63001Sierra LeoneAfrica52.9588ThailandAsia76.15840SingaporeAsia82.755647\n\n\nWe see that all of the influential points are in Africa and Asia. To compare these to their regions in particular, and to account for possible socioeconomic factors, we observe the regional mean and standard deviation for GDP per capita in Asia and Africa in 2015.\n\n\nRegionMeanStandard.DevAsia7832.18512684.597Africa2704.9803182.277\n\n\nAlmost all of the outliers are within one standard deviation of the mean GDP per capita of their region, except Singapore. We also note that the mean GDP per capita across all regions in 2015 was $12617.3, while the standard deviation was $17719.61; thus, all outliers except Singapore were also within one standard deviation of the overall mean. Therefore, we arrive at another piece of evidence in support of the claim that GDP per capita is not the most accurate or significant predictor of life expectancy.\nHaving qualitatively and quantitatively assessed the nature of the outlier datapoints, we now remove them from the training dataset, since shrinkage methods are very particular about outliers. We then refit the Full OLS model without outliers.\n\n\n\nCall:\nlm(formula = Life_expectancy ~ Region + Infant_deaths + Under_five_deaths + \n    Adult_mortality + Alcohol_consumption + Hepatitis_B + Measles + \n    BMI + Polio + Diphtheria + Incidents_HIV + GDP_per_capita + \n    Population_mln + Thinness_ten_nineteen_years + Thinness_five_nine_years + \n    Schooling + Economy_status_Developing, data = train_lifeexp_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.12070 -0.79788  0.04188  0.68388  2.77214 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          8.864e+01  2.599e+00  34.106  &lt; 2e-16 ***\nRegionAsia                           2.006e-01  4.626e-01   0.434 0.665444    \nRegionCentral America and Caribbean  1.403e+00  4.705e-01   2.982 0.003508 ** \nRegionEuropean Union                -1.062e+00  6.840e-01  -1.553 0.123259    \nRegionMiddle East                    4.894e-01  5.422e-01   0.903 0.368619    \nRegionNorth America                 -2.740e-01  1.026e+00  -0.267 0.789833    \nRegionOceania                       -1.518e+00  5.851e-01  -2.594 0.010755 *  \nRegionRest of Europe                 1.442e-01  5.574e-01   0.259 0.796264    \nRegionSouth America                  1.804e+00  5.257e-01   3.432 0.000838 ***\nInfant_deaths                       -9.166e-02  4.450e-02  -2.060 0.041689 *  \nUnder_five_deaths                   -4.017e-02  2.880e-02  -1.395 0.165786    \nAdult_mortality                     -4.880e-02  2.972e-03 -16.423  &lt; 2e-16 ***\nAlcohol_consumption                 -4.457e-02  5.020e-02  -0.888 0.376519    \nHepatitis_B                         -2.009e-02  2.450e-02  -0.820 0.414035    \nMeasles                              1.644e-02  8.427e-03   1.951 0.053552 .  \nBMI                                 -9.165e-02  8.721e-02  -1.051 0.295550    \nPolio                               -1.672e-02  2.382e-02  -0.702 0.484324    \nDiphtheria                           2.657e-02  2.824e-02   0.941 0.348874    \nIncidents_HIV                        4.374e-01  1.088e-01   4.020 0.000105 ***\nGDP_per_capita                       2.013e-05  8.703e-06   2.313 0.022523 *  \nPopulation_mln                       1.240e-03  1.013e-03   1.224 0.223483    \nThinness_ten_nineteen_years         -9.414e-01  6.477e-01  -1.453 0.148890    \nThinness_five_nine_years             8.650e-01  6.349e-01   1.362 0.175783    \nSchooling                           -1.244e-01  8.209e-02  -1.515 0.132476    \nEconomy_status_Developing           -3.593e+00  6.760e-01  -5.316 5.41e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.099 on 113 degrees of freedom\nMultiple R-squared:  0.983, Adjusted R-squared:  0.9794 \nF-statistic:   273 on 24 and 113 DF,  p-value: &lt; 2.2e-16\n\n\nThe issue of collinearity is not resolved, but removing influential points is not a typical fix for such an issue. More important is whether the removal of influential points pushed the residuals towards normality and/or constant variance. We thus retest the assumptions of ordinary least squares using diagnostics().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 1.930575, Df = 1, p = 0.1647\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.9888, p-value = 0.3309\n\n\nWe see that the left tail of the Residuals vs. Fitted plot is still largely positive, but that the spread of the rest of the data is more evenly distributed between positive and negative. The non-constant variance test returns a \\(p\\)-value of \\(p=0.1647&gt;0.05\\), meaning we fail to reject the null hypothesis of homoskedasticity. The Q-Q plot and Shapiro-Wilk’s test maintain the approximate normality of the residuals from the version of the model that included the influential points. Thus, upon removal of the influential points, the Full OLS model was able to fit all necessary assumptions."
  },
  {
    "objectID": "projects/SLMFinal.html#transformations",
    "href": "projects/SLMFinal.html#transformations",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Transformations",
    "text": "Transformations\nThe removal of influential points allows us to investigate the necessity for a transformation of the response with greater precision. Thus, since we identified a nonlinear relationship between the response and some of the predictors (most notably GDP per capita) in the exploratory data analysis, we now run the Box-Cox transformation of the response variable. The Box-Cox transformation procedure calculates the log-likelhood of the different values of \\(\\lambda\\) as in the following power transformation: \\[g_\\lambda (y)=\\begin{cases}y^\\lambda & \\lambda \\not= 0 \\\\ \\log{y} & \\lambda = 0\\end{cases}\\]\nWe use the Full OLS model with no influential points as the basis of this computation, plotting the log-likelihood function of \\(\\lambda\\) and identifying its 95% confidence interval.\n\n\n\n\n\n\n\n\n\nFirst, we note that \\(\\lambda=1\\) is not included in the 95% confidence interval, which tells us that a transformation does need to occur. The 95% confidence interval is not too wide, just over 1 in length, which tells us that there is not a large level of uncertainty in the necessary trasnformation. Finally, the estimated value of \\(\\lambda\\) is very close to 0 — which corresponds to a transformation by taking \\(\\log{y}\\). Thus, we fit and summarize the Box-Cox transformed linear model below, in which the response of life expectancy is replaced by the natural logarithm of life expectancy.\n\n\n\nCall:\nlm(formula = log(Life_expectancy) ~ Region + Infant_deaths + \n    Under_five_deaths + Adult_mortality + Alcohol_consumption + \n    Hepatitis_B + Measles + BMI + Polio + Diphtheria + Incidents_HIV + \n    GDP_per_capita + Population_mln + Thinness_ten_nineteen_years + \n    Thinness_five_nine_years + Schooling + Economy_status_Developing, \n    data = train_lifeexp_clean)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0307761 -0.0087394 -0.0003242  0.0087046  0.0309040 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          4.514e+00  3.449e-02 130.863  &lt; 2e-16 ***\nRegionAsia                           1.429e-03  6.140e-03   0.233  0.81634    \nRegionCentral America and Caribbean  1.684e-02  6.244e-03   2.696  0.00808 ** \nRegionEuropean Union                -1.713e-02  9.078e-03  -1.887  0.06179 .  \nRegionMiddle East                    5.096e-03  7.195e-03   0.708  0.48025    \nRegionNorth America                 -6.391e-03  1.361e-02  -0.469  0.63962    \nRegionOceania                       -2.284e-02  7.765e-03  -2.941  0.00397 ** \nRegionRest of Europe                -1.710e-03  7.397e-03  -0.231  0.81756    \nRegionSouth America                  2.170e-02  6.977e-03   3.110  0.00237 ** \nInfant_deaths                       -8.061e-04  5.905e-04  -1.365  0.17492    \nUnder_five_deaths                   -1.102e-03  3.822e-04  -2.883  0.00471 ** \nAdult_mortality                     -7.090e-04  3.944e-05 -17.977  &lt; 2e-16 ***\nAlcohol_consumption                 -1.422e-04  6.662e-04  -0.213  0.83139    \nHepatitis_B                         -2.868e-04  3.251e-04  -0.882  0.37960    \nMeasles                              1.515e-04  1.118e-04   1.354  0.17833    \nBMI                                 -1.200e-03  1.157e-03  -1.037  0.30194    \nPolio                               -2.083e-04  3.162e-04  -0.659  0.51143    \nDiphtheria                           3.481e-04  3.748e-04   0.929  0.35501    \nIncidents_HIV                        4.698e-03  1.444e-03   3.254  0.00150 ** \nGDP_per_capita                       1.662e-07  1.155e-07   1.439  0.15283    \nPopulation_mln                       1.484e-05  1.345e-05   1.103  0.27229    \nThinness_ten_nineteen_years         -1.310e-02  8.596e-03  -1.524  0.13035    \nThinness_five_nine_years             1.226e-02  8.426e-03   1.455  0.14853    \nSchooling                           -1.935e-03  1.089e-03  -1.776  0.07840 .  \nEconomy_status_Developing           -4.201e-02  8.971e-03  -4.683 7.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01459 on 113 degrees of freedom\nMultiple R-squared:  0.9859,    Adjusted R-squared:  0.9829 \nF-statistic: 329.7 on 24 and 113 DF,  p-value: &lt; 2.2e-16\n\n\nWe see similar strengths and weaknesses between this model and the Full OLS model: high \\(R^2\\) and low standard error indicating that the linear model is an accurate fit for the data, but a high \\(R^2\\) with a low number of significant predictors indicating collinearity. Using the function from the diagnostics section above, we may also see how the Box-Cox transformed model fits the assumptions of ordinary least squares regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 4.242568, Df = 1, p = 0.039422\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.99165, p-value = 0.5892\n\n\nIn the Residuals vs. Fitted plot above, we see a much wider dispersion of residuals in the lefthand side of the data than in the righthand side. This results in a non-constant variance test \\(p\\)-value of \\(p=0.039422&lt;0.05\\), meaning we reject the null hypothesis of homoskedasticity. The normality assumption is met by both the Q-Q plot and the Shapiro-Wilks test of normality. Thus, the Box-Cox transformation returns the model to meeting only one of the assumptions (normality) while failing to meet the other (homoskedasticity)."
  },
  {
    "objectID": "projects/SLMFinal.html#collinearity",
    "href": "projects/SLMFinal.html#collinearity",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Collinearity",
    "text": "Collinearity\nOne of the recurring issues with the various models presented in this report thus far has been a presence of collinearity, which we have identified from the combination of high \\(R^2\\) and low number of significant predictors. In this section, we will use several strategies to more formally assess collinearity in the previous models.\nTo begin, we wrote a function (cond_nums()) which calculates and displays the eigenvalues and condition numbers of the matrix \\(X^TX\\) (where \\(X\\) is the matrix of observations of predictor variables). The condition numbers \\(\\kappa_i=\\sqrt{\\frac{\\lambda_i}{\\lambda_p}}\\) (where \\(\\lambda_p\\) is the smallest eigenvalue) give an indication of the amount of collinearity between predictor variables. A condition number is considered large if \\(\\kappa_i\\geq 30\\).\nWe first find the condition numbers for the Full OLS model with no influential points below. We have chosen to evaluate this model in particular since it is our best bet so far for a model that fits assumptions and is a good fit for the data.\n\n\nEigenvalueCondition.Number74050849056.42143251.00006727341.1288215104.91641823640.9990563201.5094515134.9368935379.144340793.06637061347.323419464.98094251950.46443542.20611054572.23042677.75803545258.71161532.04933246952.30231247.17886267705.4969846.10029359355.2220513.258545012011.4912217.668815518444.5012122.428235124593.720118.441629963367.324413.050567775326.939110.605514283560.13939.713782387311.37707.2981564100729.90855.9133325111904.84052.2267879182358.32711.8612311199464.16181.5082902221575.92520.7398552316367.3471\n\n\nUnsurprisingly, all of the condition numbers are greater than 30, indicating a high level of collinearity — in particular, this means that collinearity is being caused by many different approximate linear combinations between predictors.\nAnother measure of collinearity is the Variance Inflation Factor (VIF). Since collinearity leads to unstable estimations of the coefficients \\(\\beta\\), we can observe the effect of collinearity in the expression of the variance of \\(\\hat\\beta\\) for a given predictor \\(x_j\\), \\(\\textrm{var}(\\hat\\beta_j)=\\sigma^2\\left(\\frac{1}{1-R^2_j}\\right)\\frac{1}{\\sum_i(x_{ij}-\\bar x_j)^2}\\). The variance inflation factor is derived from this expression and defined as \\((1-R_j^2)^{-1}\\) and is large if and only if \\(\\textrm{var}(\\hat\\beta_j)\\) is large.\nWe calculate the VIFs of the predictors in the Full OLS model with no influential points below.\n\n\n                                  GVIF Df GVIF^(1/(2*Df))\nRegion                      100.474932  8        1.333916\nInfant_deaths                93.055043  1        9.646504\nUnder_five_deaths            87.937560  1        9.377503\nAdult_mortality               7.899828  1        2.810663\nAlcohol_consumption           4.261877  1        2.064431\nHepatitis_B                  15.428785  1        3.927949\nMeasles                       2.013224  1        1.418881\nBMI                           3.614704  1        1.901238\nPolio                        11.808635  1        3.436369\nDiphtheria                   21.785879  1        4.667535\nIncidents_HIV                 2.539843  1        1.593689\nGDP_per_capita                2.990740  1        1.729376\nPopulation_mln                1.568307  1        1.252321\nThinness_ten_nineteen_years 820.074088  1       28.636936\nThinness_five_nine_years    806.283567  1       28.395133\nSchooling                     7.137980  1        2.671700\nEconomy_status_Developing     9.292191  1        3.048309\n\n\nA VIF of exactly 1 indicates perfectly orthogonal variables; thus, we hope for VIFs not much larger than 1. Here, we see several much larger than 1, including Infant_deaths, Under_five_deaths, Hepatitis_B, Diphtheria, Thinness_ten_nineteen_years, and Thinness_five_nine_years. Qualitatively, these make sense: counts of childhood mortality are likely collinear to each other, rates of vaccine coverage are likely collinear to each other, and measures of thinness are by definition collinear to BMI (since their calculation is based off extreme BMI values).\nWith the above two assessments, we have identified a continued presence of collinearity in our models. Thus, in the following section, we will use the model selection technique of stepwise elimination to attempt to reduce collinearity."
  },
  {
    "objectID": "projects/SLMFinal.html#model-selection",
    "href": "projects/SLMFinal.html#model-selection",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Model Selection",
    "text": "Model Selection\nModel selection techniques are used to optimally select a subset of predictors. The technique we will rely on in this section is stepwise elimination, which evaluates the AIC (Akaike’s Information Criterion) of different models using a sequential search method. We include the summary of the model chosen by stepwise elimination below. The starting model is the Full OLS model with no influential points.\n\n\n\nCall:\nlm(formula = Life_expectancy ~ Region + Infant_deaths + Under_five_deaths + \n    Adult_mortality + Measles + Incidents_HIV + GDP_per_capita + \n    Thinness_ten_nineteen_years + Thinness_five_nine_years + \n    Schooling + Economy_status_Developing, data = train_lifeexp_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.20377 -0.81907  0.00906  0.69463  2.73534 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          8.572e+01  1.235e+00  69.382  &lt; 2e-16 ***\nRegionAsia                           3.123e-01  4.525e-01   0.690 0.491454    \nRegionCentral America and Caribbean  1.334e+00  4.338e-01   3.075 0.002608 ** \nRegionEuropean Union                -1.165e+00  6.562e-01  -1.775 0.078453 .  \nRegionMiddle East                    4.969e-01  4.871e-01   1.020 0.309706    \nRegionNorth America                  2.215e-01  8.913e-01   0.248 0.804199    \nRegionOceania                       -1.521e+00  5.279e-01  -2.882 0.004685 ** \nRegionRest of Europe                 3.159e-01  5.210e-01   0.606 0.545512    \nRegionSouth America                  1.808e+00  4.957e-01   3.647 0.000394 ***\nInfant_deaths                       -8.324e-02  4.286e-02  -1.942 0.054507 .  \nUnder_five_deaths                   -4.085e-02  2.764e-02  -1.478 0.142140    \nAdult_mortality                     -4.906e-02  2.872e-03 -17.081  &lt; 2e-16 ***\nMeasles                              1.154e-02  7.599e-03   1.519 0.131467    \nIncidents_HIV                        3.989e-01  1.070e-01   3.729 0.000295 ***\nGDP_per_capita                       2.043e-05  8.613e-06   2.372 0.019309 *  \nThinness_ten_nineteen_years         -8.910e-01  6.292e-01  -1.416 0.159364    \nThinness_five_nine_years             8.518e-01  6.161e-01   1.383 0.169352    \nSchooling                           -1.531e-01  7.314e-02  -2.093 0.038473 *  \nEconomy_status_Developing           -3.774e+00  6.131e-01  -6.154 1.05e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.095 on 119 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9796 \nF-statistic: 366.6 on 18 and 119 DF,  p-value: &lt; 2.2e-16\n\n\nOn first glance, we see a greater proportion of significant predictors than in the Full OLS model, with an equivalent \\(R^2\\) and smaller residual standard errors. Thus, this model is an improvement from the OLS model.\nWe can also observe the effect of stepwise elimination on the collinearity of the model. We generate the condition numbers and VIFs of the predictors in the stepwise elimination model.\n\n\nEigenvalueCondition.Number74049520575.49374391.00004872096.5251508123.2830177993.3280916644.999340626.37258651350.07253546.28958034569.5562627.843393310860.1409412.895482013391.8649139.809826423014.008924.683903654771.445814.374121671774.548412.462430877083.201810.840497482648.79409.347803989003.34517.805623197399.63652.9741046157791.28282.3905556175999.60751.6836676209716.66790.9839026274337.4357\n\n\n                                  GVIF Df GVIF^(1/(2*Df))\nRegion                       31.081302  8        1.239599\nInfant_deaths                87.034271  1        9.329216\nUnder_five_deaths            81.675042  1        9.037425\nAdult_mortality               7.437792  1        2.727231\nMeasles                       1.649742  1        1.284423\nIncidents_HIV                 2.473247  1        1.572656\nGDP_per_capita                2.952240  1        1.718209\nThinness_ten_nineteen_years 779.870479  1       27.926161\nThinness_five_nine_years    765.094694  1       27.660345\nSchooling                     5.709675  1        2.389493\nEconomy_status_Developing     7.704286  1        2.775660\n\n\nCollinearity seems not to have improved significantly. The condition numbers are once again all greater than 30, and the VIFs that were exceptionally large continue to be so (Infant_deaths, Under_five_deaths, Thinness_ten_nineteen_years, and Thinness_five_nine_years in particular). Thus, the stepwise elimination did not resolve our problem with collinearity.\nHowever, we can combine stepwise elimination with our previous Box-Cox transformation. We display a summary of this model below.\n\n\n\nCall:\nlm(formula = log(Life_expectancy) ~ Region + Under_five_deaths + \n    Adult_mortality + Incidents_HIV + GDP_per_capita + Schooling + \n    Economy_status_Developing, data = train_lifeexp_clean)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0296250 -0.0106049  0.0006479  0.0085137  0.0285783 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          4.474e+00  1.331e-02 336.282  &lt; 2e-16 ***\nRegionAsia                           1.105e-03  5.204e-03   0.212 0.832130    \nRegionCentral America and Caribbean  1.735e-02  5.549e-03   3.126 0.002211 ** \nRegionEuropean Union                -1.534e-02  8.449e-03  -1.816 0.071864 .  \nRegionMiddle East                    2.853e-03  6.327e-03   0.451 0.652786    \nRegionNorth America                  2.165e-03  1.159e-02   0.187 0.852171    \nRegionOceania                       -2.342e-02  6.481e-03  -3.613 0.000439 ***\nRegionRest of Europe                 3.368e-03  6.660e-03   0.506 0.613968    \nRegionSouth America                  2.194e-02  6.280e-03   3.493 0.000665 ***\nUnder_five_deaths                   -1.556e-03  1.135e-04 -13.713  &lt; 2e-16 ***\nAdult_mortality                     -7.138e-04  3.714e-05 -19.219  &lt; 2e-16 ***\nIncidents_HIV                        3.933e-03  1.352e-03   2.909 0.004310 ** \nGDP_per_capita                       1.573e-07  1.134e-07   1.387 0.167841    \nSchooling                           -1.624e-03  8.670e-04  -1.873 0.063496 .  \nEconomy_status_Developing           -4.581e-02  8.048e-03  -5.693 8.67e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01447 on 123 degrees of freedom\nMultiple R-squared:  0.9849,    Adjusted R-squared:  0.9832 \nF-statistic: 573.8 on 14 and 123 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, we see very small residual standard errors, many significant predictors, and a high \\(R^2\\). Let us check the collinearity.\n\n\nEigenvalueCondition.Number74049137028.9651951.00004351935.852037130.442440903.2792351345.49141455.6810787132.2553151.97114922073.918128.98427850545.064517.39237765250.001714.35312671826.837411.94444778736.65919.70433287352.87019.31469389161.16533.553624144352.47702.459924173499.87311.119023257241.2245\n\n\n                               GVIF Df GVIF^(1/(2*Df))\nRegion                    13.851037  8        1.178535\nUnder_five_deaths          7.882325  1        2.807548\nAdult_mortality            7.119739  1        2.668284\nIncidents_HIV              2.263468  1        1.504483\nGDP_per_capita             2.929148  1        1.711475\nSchooling                  4.593577  1        2.143263\nEconomy_status_Developing  7.599101  1        2.756647\n\n\nThe condition numbers are still very large, but the VIFs have decreased notably. Thus, the combination of the absence of influential points, the Box-Cox transformation, and predictors chosen via stepwise elimination allows a reduction in collinearity. This in turn stabilizes the estimates of the coefficients, \\(\\hat\\beta\\), allowing for more accurate interpretability of the model.\nAlongside collinearity, we can also check diagnostics of both models above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 1.32929, Df = 1, p = 0.24893\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.98694, p-value = 0.2166\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 2.851855, Df = 1, p = 0.091269\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.98247, p-value = 0.07385\n\n\nWe see that both stepwise-elimination-chosen models fit the necessary assumptions, as demonstrated by the plots and \\(p\\)-values greater than 0.05 above. Thus, while not fully eliminating collinearity, the model selection via stepwise elimination did result in the model’s adherence to assumptions."
  },
  {
    "objectID": "projects/SLMFinal.html#shrinkage-methods",
    "href": "projects/SLMFinal.html#shrinkage-methods",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nTo accomplish our secondary goal of model accuracy when faced with new data, we consider different shrinkage methods for linear models.\nTo begin, we use principal component analysis to construct a regression model. This method is useful for our modelling purposes because of the level of collinearity between predictors. Principal component analysis/regression builds a model based on principal components, which are representations of the variation in the data. Since principal components are orthogonal, collinearity is resolved.\nWe begin by generating the principal components from the training dataset. The standard deviation of each component is listed below.\n\n\n [1] 18694.096   159.949    82.629    24.598    15.799    11.650     4.836\n [8]     4.230     3.128     3.062     1.973     1.500     1.284     1.254\n[15]     1.107     0.470     0.300     0.000\n\n\nWe see a steep dropoff in magnitude of the first and second components’ standard deviations (the first component’s standard deviation is 116.8754 times the second’s). The drop between the second and third standard deviations is much less sizeable (the second is less than twice the third). In comparison, the remaining components account for a miniscule amount of all variation present in the training dataset. We can view this as well in the Scree Plot below, which plots the standard deviation of the principal components against their component number.\n\n\n\n\n\n\n\n\n\nFrom the Scree Plot and the standard deviations themselves, we believe a cutoff of 2 components is sufficient for our model. We thus build a principal component regression (PCR) model, a summary of which is displayed below.\n\n\nData:   X dimension: 138 24 \n    Y dimension: 138 1\nFit method: svdpc\nNumber of components considered: 2\nTRAINING: % variance explained\n                      1 comps  2 comps\nX                       99.99   100.00\nlog(Life_expectancy)    36.20    36.52\n\n\nAs expected, we see that 100% of the variance of all predictors is explained with only two components.\nWe also consider partial least squares (PLS) regression. While PCR produces linear combinations of predictors that minimize collinearity by tending towards orthogonality, it does not consider the response variable in its construction of linear combinations. Partial least squares does. Thus, we build a model with all predictors, and include the Box-Cox transformation, which has been shown to have improved fit in previous sections.\n\n\n\n\n\n\n\n\n\nThe above plot displays the number of components included in the partial least squares regression against their respective cross-validated error (in root mean squared error). We once again see a steep dropoff with the inclusion of only a few components. In particular, the error seems to stabilize and minimize with four components.\nFinally, we consider ridge regression. Ridge regression is particularly useful when faced with unstable, collinear coefficients \\(\\hat\\beta\\). It works by normalizing the regression coefficients, then minimizing the normalized coefficients. Formally, ridge regression seeks to minimize \\(\\beta\\) in the expression \\[(y-X\\beta)^T(y-X\\beta)+\\lambda\\sum_j\\beta_j^2\\] for some \\(\\lambda\\geq 0\\). We build the ridge regression model and return the value of \\(\\lambda\\) that minimizes this expression for our specific dataset.\n\n\n 1.40703518 \n         29 \n\n\nWe see that a value of \\(\\lambda\\approx 1.4\\) minimizes the normalized \\(\\hat\\beta\\).\nWe will compare the efficacy of each model via their root mean squared error in the final section. However, to conclude this section, it is important to mention the limitations of the shrinkage methods described here. In general, they reduce the amount of explanatory power given by a model. Individual coefficients lose their attachment to real-world variables. Thus, while typically powerful prediction techniques, shrinkage methods come at the cost of reduced descriptive capabilities. Additionally, shrinkage methods only work with purely numeric predictors, meaning we have lost the ability to include the categorical variable Region in these models. In the following section, a model will be considered built solely on this categorical variable via one-way ANOVA."
  },
  {
    "objectID": "projects/SLMFinal.html#anova",
    "href": "projects/SLMFinal.html#anova",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA) models can be used to quantify relationships between different groups/levels of categorical variables. In particular, they can characterize the proportional size of within-group variation versus between-group variation. They are constructed the same way as linear regression models, with the condition that all predictors must be categorical. In this section, we will begin by visually assessing ANOVA of one or multiple factors, then build the most suitable ANOVA model.\nWith regards to our dataset, there are two categorical variables of interest: Region (one of 9 geopolitical regions) and Economy_status_developing (binary, 1 = developing country and 0 = developed country). We are interested in whether some level of variation in the response, life expectancy, can be attributed to these variables alone. In other words, we would like to see whether being a citizen of “developed” or “developing” country can predict your life expectancy, and the equivalent for region. We will look both at one-way ANOVA (treating each variable in isolation) and two-way ANOVA (allowing for interactions of the two variables).\nTo start, let us visualize the between-group variation with box plots. Below we observe the box-and-whisker plot of life expectancy grouped by region.\n\n\n\n\n\n\n\n\n\nWhile some regions seem to have notable differences in life expectancy (e.g. the jump from Africa to Oceania, or from Oceania to European Union), most immediate jumps between groups seem similar. Nevertheless, there could be some meaningful relationship between the variable’s possible values. Let us now observe life expectancy grouped by country’s economic status.\n\n\n\n\n\n\n\n\n\nWe see what could be a meaningful difference between the two groups. However, the large variance of the “developing” group compared to the “developed” group is troubling for modelling purposes.\nNow, let us consider visual plots displaying the interaction between region and economic status.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn both plots, we see an issue that would affect the two-way ANOVA. In the first plot, it is clear that there is not representation of both economy statuses in each region, so a full analysis of the interaction between the variables would be impossible. This pattern carries to the second plot. Thus, we will not build a two-way ANOVA model.\nOf the two one-way ANOVA models, the model predicting by region seems more promising. Thus, we provide a summary of that model below.\n\n\n\nCall:\nlm(formula = Life_expectancy ~ Region, data = train_lifeexp_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.9600  -3.0275  -0.0863   2.2350  13.2400 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          62.8600     0.8282  75.903  &lt; 2e-16 ***\nRegionAsia                            8.7926     1.3962   6.298 4.37e-09 ***\nRegionCentral America and Caribbean  10.5525     1.4786   7.137 6.16e-11 ***\nRegionEuropean Union                 17.0192     1.2985  13.107  &lt; 2e-16 ***\nRegionMiddle East                    11.4500     1.7568   6.518 1.46e-09 ***\nRegionNorth America                  15.5400     3.5621   4.363 2.61e-05 ***\nRegionOceania                         9.0400     1.8311   4.937 2.41e-06 ***\nRegionRest of Europe                 14.1554     1.5913   8.895 4.48e-15 ***\nRegionSouth America                  11.3900     1.7568   6.483 1.74e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.899 on 129 degrees of freedom\nMultiple R-squared:  0.6157,    Adjusted R-squared:  0.5918 \nF-statistic: 25.83 on 8 and 129 DF,  p-value: &lt; 2.2e-16\n\n\nWe see a model with only significant predictor values, but with a much lower \\(R^2\\) and much higher residual standard errors than our other models. As a secondary measure of the model’s performance, we test its adherence to assumptions via the diagnostics() function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 22.82666, Df = 1, p = 1.7729e-06\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm)\nW = 0.97403, p-value = 0.009775\n\n\n\n\n\nDfF valuePr(&gt;F)\n\n\n\n81.940.059\n\n129       \n\n\n\n\nEven accounting for the discrete levels of the model, the variance does not seem constant: in the residuals vs. fitted plot, the residuals for smallest fitted values are much larger than those for the largest fitted values. Similarly, we see large tails tending away from the desired line on the Q-Q plot. The small \\(p\\)-values on the non-constant variance test and Shapiro-Wilks test of normality could be attributed to the discrete values of the predictor variable. However, we appeal as well to Levene’s test for Homogeneity of Variance, an equivalent to the non-constant variance test built for discrete variables. The small \\(p\\)-value from this test confirms that this model does not fit the assumptions.\nThus, to conclude this section, we note that the ANOVA model is the second-worst of the models built in this report on all essential fronts. In short, region is not a sufficient predictor (on its own) of life expectancy."
  },
  {
    "objectID": "projects/SLMFinal.html#results-evaluating-model-performance",
    "href": "projects/SLMFinal.html#results-evaluating-model-performance",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Results: Evaluating Model Performance",
    "text": "Results: Evaluating Model Performance\nBelow is included a table that summarizes the performance of each model built in this paper. We include several measures that paint a picture of the model’s accuracy from several angles. In particular, we include the root mean squared error, defined as \\(\\sqrt{\\sum_{i=1}^n (\\hat y_i- y_i)^2/n}\\), where \\(y_i\\) is the observed value of the response \\(y\\), \\(\\hat y_i\\) is the corresponding fitted value, and \\(n\\) is the sample size. This is commonly used as a measure of performance for models that do not have associated \\(R^2\\), adjusted \\(R^2\\), or residual standard errors, like the shrinkage methods used in this paper.\nThe columns are as follows: R.Squared corresponds to the multiple \\(R^2\\) value as given in the summary of the linear model; Adj.R.Squared corresponds to the adjusted \\(R^2\\) of the model; RSE corresponds to the residual standard error of the model; RMSE.train corresponds to the root mean squared error between the fitted values and the observed values in the training dataset; RMSE.test corresponds to the root mean squared error between the fitted values and the observed values in the testing dataset; and RMSE.ratio is a measure of the difference between the two other RMSE values, calculated by dividing RMSE.test by RMSE.train. The shrinkage methods do not have associated values for some of these performance metrics, and thus no associated value is listed for those models.\n\n\nModelR.SquaredAdj.R.SquaredRSERMSE.trainRMSE.testRMSE.ratioFull OLS0.9800.9761.2251.1131.5051.352SLR (GDP/cap only)0.3960.3926.1336.0906.0180.988Full OLS (no influ. pts.)0.9830.9791.09911.0561.9900.180Box-Cox Trans.0.9860.9830.01568.10865.7220.965Stepwise Elim.0.9820.9801.09511.0601.9390.175Step Elim. & Box-Cox0.9850.9830.01468.10865.7240.965PCR68.10365.7210.965PLS68.09865.7240.965Ridge Regression1.2851.6521.286ANOVA0.6160.5924.89910.6384.5670.429\n\n\nThere are roughly three groups of models analyzed: standard models and their variations, shrinkage models, and the ANOVA model. As explained in the ANOVA section, the ANOVA model is the second-worst of all models, so in terms of picking the best model, we will only consider candidates from the first two groups. For the shrinkage method models, a best model is easy to select: the ridge regression has the smallest RMSE for both training and testing datasets.\nHowever, a best model is more difficult to select from the standard models and their variations. The models with Box-Cox transformation applied maximize \\(R^2\\) and adjusted \\(R^2\\), while minimizing residual standard error. However, their RMSE are the largest of all models on both training and testing sets. The stepwise elimination model without the Box-Cox transformation also emerges as a possible best model due to its low residual standard error, high \\(R^2\\) and adjusted \\(R^2\\), and low RMSE, especially when faced with new (test) data.\nTo help select a best of the standard models, we consider whether or not each model meets the assumptions of linear regression. The adherence of each model to the assumptions of homoskedasticity and normality of residuals is summarized in the table below.\n\n\nModelHomoskedasticityNormalityFull OLSfalsetrueSLR (GDP/cap only)truefalseFull OLS (no influential points)truetrueBox-Cox TransformationfalsetrueStepwise EliminationtruetrueBox-Cox & Step. Elim.truetrueOne-Way ANOVAfalsefalse\n\n\nOnly three models meet both assumptions: the Full OLS model with no influential points and both stepwise elimination models. Since the stepwise elimination model without the Box-Cox transformation meets both assumptions and does very good on its performance measures, we choose that model as the best of the non-shrinkage models."
  },
  {
    "objectID": "projects/SLMFinal.html#conclusion",
    "href": "projects/SLMFinal.html#conclusion",
    "title": "Prediction of Life Expectancy and its Relationship to GDP",
    "section": "Conclusion",
    "text": "Conclusion\nTo conclude, we summarize our questions and the answers we were able to discern. Our first question was whether or not GDP per capita has a linear relationship with life expectancy. While there certainly was some form of relationship between the two that could be expressed as linear, life expectancy was better predicted by GDP per capita in conjunction with other predictors. Our second question was which model is best for predicting the response, life expectancy. Our two contenders are the ridge regression model and the stepwise elimination model (with transformation of the response selected via Box-Cox). Which model should be considered best mostly rests on the use of the model. If the goal is solely to predict life expectancy, ridge regression is likely best, due to its minimization properties. However, ridge regression loses explanatory power by the normalization of coefficients inherent to the method. Thus, if prediction of life expectancy in conjunction with coefficients that bear some explanatory power over future observational data, the stepwise elimination model is best. With this, we have answered both questions we sought to resolve at the outset of our project."
  },
  {
    "objectID": "projects/dssi.html",
    "href": "projects/dssi.html",
    "title": "Data Science for Social Impact: Global Tuberculosis Mortality",
    "section": "",
    "text": "Charlotte and I were thrilled to win second place at Harvey Mudd College’s inaugural Data Science for Social Impact (DSSI) Datathon!\n\n\n\n\n\n\n\n\n\n\nWe were given 4 hours to explore the World Bank’s Millennium Development Goals dataset — a rich but messy collection covering 263 countries and regions between 2006 and 2015 — and build a stunning visualization addressing the success of the Millennium Development Goals. Focusing on the 6th goal (combat HIV/AIDS, malaria, and other diseases) and drawing inspiration from a new John Green book, we focused on global incidence of tuberculosis (TB). We chose to present our visualization via an interactive Shiny dashboard to explore how TB death rates correlate with health, education, and economic proxy variables.\n\nWe found some expected trends, like a correlation between TB incidence and TB mortality. However, TB treatment success rates had almost no correlation with TB mortality. Rather, detection of TB was more influential. Since the disease has variable rates of progression, detection is often the most important step, as it unlocks various kinds of treatment. Early detection is also an indication of well-established, accessible healthcare systems. This observation is corroborated by the negative correlation between TB deaths and the percent of births attended by skilled medical staff.\nThe final product of is currently hosted on Charlotte’s website."
  },
  {
    "objectID": "projects/phys070.html",
    "href": "projects/phys070.html",
    "title": "Efficacy of STEM Community Learning Practices",
    "section": "",
    "text": "We sought to understand the ways in which disparate community-based learning methods commonly employed in Pomona College STEM courses impacted student perceptions of comprehension and belonging. We designed an online survey to measure these perceptions which received nearly 130 responses (with a total student population of 1,732). The poster below details our survey design, data analysis, and conclusions. We presented our results at the 2023 IDEAL Symposium.\nDownload PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emrys King",
    "section": "",
    "text": "Hello! Welcome to my website.\nI am a postgraduate student at Imperial College London studying towards an MSc Statistics. I received my Bachelor’s of Arts from Pomona College in Claremont, California, with majors in Mathematics (Statistics) and Spanish.\nI am passionate about transforming data into communicative, actionable analyses. My experience in mathematics research provides a foundation for analyzing all types of data, while my background in languages propels my desire to communicate data effectively. I am happiest with my work when complex analysis is blended with beautiful visualizations."
  }
]